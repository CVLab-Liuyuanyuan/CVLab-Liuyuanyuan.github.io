
<html xmlns="http://www.w3.org/1999/html"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <meta name="keywords" content="刘袁缘, 中国地质大学（武汉）, Yuanyuan Liu, machine learning, CUG, China University of Geosciences">
    <meta name="description" content="Liu Yuanyuan&#39;s home page">
    <link href="main_style.css" media="all" rel="stylesheet">
    <title>Liu Yuanyuan  - Homepage</title>
</head>

<body>
<!--<div id="navbar" align=right>-->
<!--    <a href="/">HOME</a>-->
<!--    <a href="publications.html">PUBLICATIONS</a>-->
<!--    <a href="teaching.html">TEACHING</a>-->
<!--    <a href="readinglist.html">READING</a>-->
<!--    <a href="conf.html">RESOURCES</a>-->
<!--</div>-->
<body>
    <span id="clock"></span>
</body>
<hr>
<br><br>
<table id="tbInformation" width="100%">
    <tbody>
    <tr>
        <td width="500">
            <!--            <h1>YUANYUAN LIU <img src="indexfiles/blank.png"></h1>-->
            <a href="http://grzy.cug.edu.cn/liuyuanyuan/zh_CN/index.htm" target="_blank"><h1>YUANYUAN LIU <img src="indexfiles/blank.png"></h1></a>
            <!--            <h1><img src="indexfiles/blank.png"></h1>-->
        </td>
        <td rowspan="3" align="center">
            <a href="http://grzy.cug.edu.cn/liuyuanyuan/zh_CN/index.htm" target="_blank"><img src="indexfiles/liuyuanyuan.png" border="0" width="300" height="300"></a>
        </td>
    </tr>


    <tr>
        <td>
            <h3>Associate Professor</br>
                School of Information and Engineering</br>
                China University of Geosciences (Wuhan)</br>
                <p>No. <font face="Times New Roman">68</font> Jincheng Road,Wuhan, P.R. China Postcode: <font face="Times New Roman">430074.</font></p>
                <strong>Telephone:<font face="Times New Roman"> 86-013349830890</font></br></strong>
                <strong>Email:</strong> liuyy@cug.edu.cn</br></h3>
        </td>
    </tr>

    </tbody>
</table>
<!--<table width="100%">
    <tbody>
    <tr>
        <td><font color="#0000FF"><strong>I am looking for new MS/BS students to join my research project working on machine learning and computer vision. If you have a <font color="#FF0000">good personality & solid mathematical & English background and happen to be interested in my research</font>, please contact me!</strong></font>
        </td>
    </tr>
    </tbody>
</table>-->

<hr>


<h3>Research Area</h3>
<h4>My research interests fall into the areas of <font color="red">computer vision, emotion computing, image and video understanding,</font> etc. More specifically, I am interested in the following topics:</h4>

<ol id="ProjectsList" style="line-height: 120%">
    <li>Head pose estimation, facial expression recognition, facial AU detection, and facial feature localization in unconstrained environment</li>
    <li>Human computer interface including emotion computing, group emotion recognition, and multi-model emotion recognition</li>
    <li>Image/Video understanding including scene recognition, object detection, and action recognition</li>

</ol>
<hr>

<h3>RECENT NEWS:</h3>
<p>
<ol id="NewsOL">
    <!--    <li>2021-05-08: One co-authored paper was accepted by ICML 2021.</li>-->
    <!--    <li>2021-04-30: One paper was accepted by IJCAI 2021.</li>-->
    <!--    <li>2021-04-06: Invitation to be a technical program committee member (TPC) for NeurIPS 2021.</li>-->
    <!--    <li>2021-03-23: One paper was accepted by IEEE Transactions on Multimedia.</li>-->
    <!--    <li>2021-03-06: One paper was accepted by ICME 2021 for oral representation.</li>-->
    <!--    <li>2020-12-17: One paper was accepted by IEEE Transactions on Knowledge and Data Engineering.</li>-->
    <!--    <li>2020-12-14: I have been invited as a TPC for ICML 2021.</li>-->
    <!--    <li>2020-11-03: I have been invited as an Area Chair for ICME 2021.</li>-->
    <!--    <li>2020-10-28: I have been invited as an Associate Editor for BMC Bioinformatics (IF 3.3).</li>-->
    <!--    <li>2020-10-17: I have been invited to be a senior technical program committee member (SPC) for IJCAI 2021.</li>-->
    
    <li><p>Attended the 32nd  <b>ACM Multimedia</b> Conference, 28 October - 1 November 2024 • Melbourne, Australia</p></li>
    <img src="image/news/ACMM2024.jpg" width="290px" onclick="window.open('image/news/ACMM2024.jpg', '_blank')">
    
    <li><p>Taking part in <b>Valse</b> 2023, 10-12 June 2023 • WuXi, China</p></li>
    <img src="image/news/valse2023.jpg" width="290px" onclick="window.open('image/news/valse2023.jpg', '_blank')">
    
    <li><p>Five papers from our group accepted, February 2023 • One accepted by <b>CVPR 2023</b></p></li>
    <img src="image/news/paper.jpg" width="290px" onclick="window.open('image/news/paper.jpg', '_blank')">
    
    <li><p>Presented at <b>ACM Multimedia</b> 2022, 10-14 October 2022 • Lisbon, Portugal</p></li>
    <img src="image/news/acmm2022.jpg" width="290px" onclick="window.open('image/news/acmm2022.jpg', '_blank')">
    
    <li><p>Taking part in <b>Valse</b> 2021, 8-10 October 2021 • Hangzhou, China</p></li>
    <img src="image/news/valse2021.jpg" width="290px" onclick="window.open('image/news/valse2021.jpg', '_blank')">
    
    <li><p>Presentation at <b>ICIP</b> 2021, 19-22 September 2021 • Anchorage, Alaska, USA</p></li>
    

</ol>
</p>
<hr>


<h3>Publication</h3>
<table id="tbPublications">
    <tbody>
    <!--    <tr><td><strong>Pre-print</strong></td></tr>-->
    <!--    <tr><td><hr></td><td><hr></td></tr>-->


    <!--    <tr><td><strong>2021</strong></td></tr>-->
    <!--    <tr><td><hr></td><td><hr></td></tr>-->
    <tr><td><strong>Journal Paper:</strong></td></tr>
    <tr><td><br></td></tr>

    <tr>
        <td width="306"><hr></td>
        <td><hr></td>
    </tr>
    <tr><td><br></td></tr>
        
    <tr>
        <td width="306"><img src="image/journalpaper/ECPA.png" width="290px"></td>
        <td style="line-height: 120%">
            Yu Wang, <b>Yuanyuan Liu</b>, Shunping Zhou, Yuxuan Huang, Chang Tang, Wujie Zhou, Zhe Chen. "Emotion-oriented Cross-modal Prompting and Alignment for Human-centric Emotional Video Captioning." IEEE Transactions on Multimedia.[<a href="pdf/journalpaper/Emotion-oriented Cross-modal Prompting and Alignment for Human-centric Emotional Video Captioning.pdf" target="_blank">pdf</a>]
<!--             [<a href="" target="_blank">doi</a>] -->
            [<a href="https://github.com/virtuesvvy/ECPA_Emotional_Video_Captioning" target="_blank">code(github)</a>]
        </td>
    </tr>
    <tr><td><br></td></tr>

    <tr>
        <td width="306"><img src="image/journalpaper/Multitarget_Domain_Adaptation_Building_Instance_Extraction_of_Remote_Sensing_Imagery_With_Domain-Common_Approximation_Learning.png" width="290px"></td>
        <td style="line-height: 120%">
            Fayong Zhang, Kejun Liu, <b>Yuanyuan Liu</b>, Chaofan Wang, Wujie Zhou, Hongyan Zhang, Lizhe Wang. "Multi-target Domain Adaptation Building Instance Extraction of Remote Sensing Imagery with Domain-common Approximation learning." IEEE Transactions on Geoscience and Remote Sensing (2024).[<a href="pdf/journalpaper/Multitarget_Domain_Adaptation_Building_Instance_Extraction_of_Remote_Sensing_Imagery_With_Domain-Common_Approximation_Learning.pdf" target="_blank">pdf</a>]
            [<a href="https://www.sciencedirect.com/science/article/pii/S0952197624005062" target="_blank">doi</a>]
            [<a href="https://github.com/kejun1/DAL/tree/main" target="_blank">code(github)</a>]
        </td>
    </tr>
    <tr><td><br></td></tr>

        
    <tr>
        <td width="306"><img src="image/journalpaper/Token-disentangling.png" width="290px"></td>
        <td style="line-height: 120%">
            Guanghao Yin, <b>Yuanyuan Liu</b>, Tengfei Liu, Haoyu Zhang, Fang Fang, Chang Tang, Liangxiao Jiang."Token-disentangling Mutual Transformer for multimodal emotion recognition Engineering." Applications of Artificial Intelligence Volume 133, Part D, July 2024, Page 108348[<a href="pdf/journalpaper/Token-disentangling.pdf" target="_blank">pdf</a>]
            [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10471580" target="_blank">doi</a>]
            [<a href="https://github.com/cug-ygh/TMT" target="_blank">code(github)</a>]
        </td>
    </tr>
    <tr><td><br></td></tr>    
        
    <tr>
        <td width="306"><img src="image/journalpaper/APSL Action-positive separation learning for unsupervised temporal action localization.png" width="290px"></td>
        <td style="line-height: 120%">
            <b>Yuanyuan Liu</b>, Ning Zhou, Fayong Zhang, Wenbin Wang, Yu Wang, Kejun Liu, Ziyuan Liu."APSL: Action-positive separation learning for unsupervised temporal action localization."Information Sciences,Volume 630,2023,Pages 206-221.[<a href="pdf/journalpaper/APSL Action-positive separation learning for unsupervised temporal action localization.pdf" target="_blank">pdf</a>]
            [<a href="https://doi.org/10.1016/j.ins.2023.02.047" target="_blank">doi</a>]
            [<a href="https://github.com/bugcat9/APSL" target="_blank">code(github)</a>]
        </td>
    </tr>
    <tr><td><br></td></tr>     
    
        
    <tr>
        <td width="306"><img src="image/journalpaper/Expression Snippet Transformer for Robust Video-based Facial Expression Recognition.png" width="290px"></td>
        <td style="line-height: 120%">
            <b>Yuanyuan Liu</b>, Wenbin Wang, Chuanxu Feng, Haoyu Zhang, Zhe Chen, Yibing Zhan."Expression snippet transformer for robust video-based facial expression recognition."Pattern Recognition,Volume 138,2023,109368.[<a href="pdf/journalpaper/Expression Snippet Transformer for Robust Video-based Facial Expression Recognition.pdf" target="_blank">pdf</a>]
            [<a href="https://doi.org/10.1016/j.patcog.2023.109368" target="_blank">doi</a>]
            [<a href="https://github.com/DreamMr/EST" target="_blank">code</a>]
        </td>
    </tr>
    <tr><td><br></td></tr>
        
    <tr>
        <td width="306"><img src="image/journalpaper/ConGNN%20Context-consistent%20cross-graph%20neural%20network%20for%20group%20emotion%20recognition%20in%20the%20wild.jpg" width="290px"></td>
        <td style="line-height: 120%">
            Yu Wang, Shunping Zhou, <b>Yuanyuan Liu</b>, Kunpeng Wang, Fang Fang, Haoyue Qian. ConGNN Context-consistent cross-graph neural network for group emotion recognition in the wild, Information Sciences (2022).[<a href="pdf/journalpaper/ConGNN%20Context-consistent%20cross-graph%20neural%20network%20for%20group%20emotion%20recognition%20in%20the%20wild.pdf" target="_blank">pdf</a>]
            [<a href="https://doi.org/10.1016/j.ins.2022.08.003" target="_blank">doi</a>]
            [<a href="https://github.com/virtuesvvy/ConGNN-master" target="_blank">code</a>]
            [<a href="https://pan.baidu.com/s/1gzxyvysfVUdFn73cA89-DA?pwd=u3f9" target="_blank">SiteGroEmo Dataset</a>]<br>
            <b>Note</b>: The data are passcode protected. Please download and send the signed [<a href="https://pan.baidu.com/s/1c-Exs52fkFIIeGmbV40H2A?pwd=dfnm" target="_blank">EULA</a>] to <a href="mailto:qianying@cug.edu.cn">qianying@cug.edu.cn</a>
 for access request.
        </td>
    </tr>
    <tr><td><br></td></tr>


    <tr>
        <td width="306"><img src="image/journalpaper/Clip-aware Expressive Feature Learning for Video-based Facial Expression.png" width="290px"></td>
        <td style="line-height: 120%">
            <b>Yuanyuan Liu</b>, Chuanxu Feng, Xiaohui Yuan, Lin Zhou, Wenbin Wang, Jie Qin, Zhongwen Luo. Clip-aware Expressive Feature Learning for Video-based Facial Expression Recognition, Information Sciences (2022).[<a href="pdf/journalpaper/Clip-aware Expressive Feature Learning for Video-based Facial Expression.pdf" target="_blank">pdf</a>]
            [<a href="https://doi.org/10.1016/j.ins.2022.03.062" target="_blank">doi</a>]
            [<a href="https://github.com/CVLab-Liuyuanyuan/CEFLNet" target="_blank">code</a>]
        </td>
    </tr>
    <tr><td><br></td></tr>

    <tr>
        <td width="306"><img src="image/journalpaper/DML-Net.png" width="290px"></td>
        <td style="line-height: 120%">
            <b>Yuanyuan Liu</b>, Wei Dai, Fang Fang, Yongquan Chen, Rui Huang, et al.  "Dynamic multi-channel metric network for joint pose-aware and identity-invariant facial expression recognition." Information Sciences 578.(2021).[<a href="pdf/journalpaper/DML-Net.pdf" target="_blank">pdf</a>]
            [<a href="https://doi.org/10.1016/j.ins.2021.07.034" target="_blank">doi</a>]
        </td>
    </tr>
    <tr><td><br></td></tr>

    <tr>
        <td width="306"><img src="image/journalpaper/remotesense.png" width="290px"></td>
        <td style="line-height: 120%">
            <b>Yuanyuan Liu</b>, Wenbin Wang, Fang Fang, Lin Zhou, Chenxing Sun, Ying Zheng, and Zhanlong Chen.  "CscGAN: Conditional Scale-Consistent Generation Network for Multi-Level Remote Sensing Image to Map Translation" Remote Sensing 13, no. 10: 1936.[<a href="pdf/journalpaper/remotesensing-13-01936-v2_.pdf" target="_blank">pdf</a>]
            [<a href="https://doi.org/10.3390/rs13101936" target="_blank">doi</a>]
        </td>
    </tr>
    <tr><td><br></td></tr>

    <tr>
        <td width="306"><img src="image/journalpaper/TSingNet.jpg" width="290px"></td>
        <td style="line-height: 120%">
            <b>Yuanyuan Liu</b>, Jiyao Peng, Jing-Hao Xue, Yongquan Chen, Zhang-Hua Fu, "TSingNet: Scale-aware and Context-rich Feature Learning for Traffic Sign Detection and Recognition in the Wild",Neurocomputing, 2021, 447(4).
            [<a href="https://pan.baidu.com/s/10XLQDjzRt3fdLnwBLHkusA" target="_blank">code(baiducloud password:code)</a>]
            [<a href="https://github.com/Lionel-McCree/TsingNet" target="_blank">code(github)</a>]
        </td>
    </tr>
    <tr><td><br></td></tr>

    <!--    <tr>-->
    <!--        <td width="306"><img src="image/journalpaper/joint%20spatial.jpg" width="290px"></td>-->
    <!--        <td>-->
    <!--            <b>Yuanyuan Liu</b>, Jiyao Peng, Jiabei Zeng, Shiguang Shan, "Joint Spatial and Scale Attention Network for Multi-view Facial Expression Recognition", IEEE Transaction on Image Processing (Pattern recognition), 2019. (under view).-->
    <!--        </td>-->
    <!--    </tr>-->
    <!--    <tr><td><br></td></tr>-->

    <tr>
        <td width="306"><img src="image/journalpaper/multi-scale%20u-shaped%20cnn.png" width="290px"></td>
        <td style="line-height: 120%">
            <b>Yuanyuan Liu</b>, Dingyuan Chen, Ailong Ma, Yanfei Zhong, Fang Fang, Kai Xu "A Multi-Scale U-Shaped CNN Building Instance Extraction Framework With Edge Constraint for High Spatial Resolution Remote Sensing Imagery", IEEE Transaction on Geosciences and Remote Sensing (TGRS), 2020, 10.1109/TGRS.2020.3022410.[<a href="pdf/journalpaper/multi-scale%20u-shaped%20cnn.pdf" target="_blank">pdf</a>]
        </td>
    </tr>
    <tr><td><br></td></tr>

    <tr>
        <td width="306"><img src="image/journalpaper/conditional%20cnn%20etc.jpg" width="290px"></td>
        <td style="line-height: 120%">
            <b>Yuanyuan Liu</b>, Xiaohui Yuan, Xi Gong, Zhong Xie, Fang Fang, Zhongwen Luo,  "Conditional convolution neural network enhanced random forest for facial expression recognition", Pattern Recognition, Volume 84, 2018, Pages 251-261.[<a href="pdf/journalpaper/conditional%20cnn%20etc.pdf" target="_blank">pdf</a>][<a href="https://doi.org/10.1016/j.patcog.2018.07.016" target="_blank">doi</a>]
        </td>
    </tr>
    <tr><td><br></td></tr>

    <tr>
        <td width="306"><img src="image/journalpaper/visual%20focus%20of%20attention.jpg" width="290px"></td>
        <td style="line-height: 120%">
            <b>Yuanyuan Liu</b>, Xinmei Li, Mulan Zhang, Fang Fang, Zhizhong Zeng. "Visual Focus of Attention and Spontaneous Smile Recognition based on Continuous Head Pose Estimation by Cascaded Multi-task Learning", International Journal of Pattern Recognition and Artificial Intelligence, 2018, accepted.[<a href="pdf/journalpaper/visual%20focus%20of%20attention.pdf" target="_blank">pdf</a>]
        </td>
    </tr>
    <tr><td><br></td></tr>

    <tr>
        <td width="306"><img src="image/journalpaper/Student%20engagement%20study.jpg" width="290px"></td>
        <td style="line-height: 120%">
            <b>Yuanyuan Liu</b>, Jingying Chen, Mulan Zhang, Chuan Rao. "Student Engagement Study based on Multi-cue detection and recognition in an Intelligent Learning Environment", Multimedia tools and applications, 2018, 1-27.[<a href="pdf/journalpaper/Student%20engagement%20study.pdf" target="_blank">pdf</a>]
        </td>
    </tr>
    <tr><td><br></td></tr>

    <tr>
        <td width="306"><img src="image/journalpaper/Deep%20Salient%20Feature%20Based%20Anti-Noise%20Transfer.jpg" width="290px"></td>
        <td style="line-height: 120%">
            Xi Gong, Zhong Xie, <b>Yuanyuan Liu*</b>, Zhuo ZHENG. "Deep Salient Feature based Transfer Convolutional Neural Network for Scene Classification of Remote Sensing Imagery", Remote Sensing 2018, 10(3), 410; [<a href="pdf/journalpaper/Deep%20Salient%20Feature%20Based%20Anti-Noise%20Transfer.pdf" target="_blank">pdf</a>]
        </td>
    </tr>
    <tr><td><br></td></tr>

    <tr>
        <td width="306"><img src="image/journalpaper/JointMultiＧscaleConvolutionNeuralNetwork.jpg" width="290px"></td>
        <td style="line-height: 120%">
            Zhuo Zheng, Fang Fang, <b>Yuanyuan Liu*</b>, Gong Xi, Guo Mingqiang, Luo Zhongwen. "Joint Multi-scale Convolution Neural Network for Scene Classification of High Resolution Remote Sensing Imagery". Acta Geodaetica et Cartographica Sinica, 2018, 47(5): 620-630. (in Chinese)[<a href="pdf/journalpaper/JointMultiＧscaleConvolutionNeuralNetwork.pdf" target="_blank">pdf</a>]
        </td>
    </tr>
    <tr><td><br></td></tr>

    <tr>
        <td width="306"><img src="image/journalpaper/Multi-level%20structured%20hybrid%20forest.jpg" width="290px"></td>
        <td style="line-height: 120%">
            <b>Yuanyuan Liu</b>, Zhong Xie, Xiaohui Yuan, Jingying Chen, Wu Song. "Multi-level Structured Hybrid Forest for Joint Head Detection and Pose Estimation". Neurocomputing, 266(11), 206-215 2017.[<a href="pdf/journalpaper/Multi-level%20structured%20hybrid%20forest.pdf" target="_blank">pdf</a>]
        </td>
    </tr>
    <tr><td><br></td></tr>

    <tr>
        <td width="306"><img src="image/journalpaper/Conditional%20Iteration%20Updated%20Random%20Forests.jpg" width="290px"></td>
        <td style="line-height: 120%">
            <b>Yuanyuan Liu</b>, Zhong Xie, Shunping Zhou, Zheng Liu, Weiming Wang, Xiuping Liu, Wei Rao. "Conditional Iteration Updated Random Forests for Unconstrained Facial Feature Location", Journal of computer aided design and graphics, Vol.29, No.10, 1881-1890 , 2017. (in Chinese)[<a href="pdf/journalpaper/Conditional%20Iteration%20Updated%20Random%20Forests.pdf" target="_blank">pdf</a>]
        </td>
    </tr>
    <tr><td><br></td></tr>

    <tr>
        <td width="306"><img src="image/journalpaper/Robust%20head%20pose%20estimation.jpg" width="290px"></td>
        <td style="line-height: 120%">
            <b>Yuanyuan Liu</b>, Jingying Chen, Zhiming Su, et al., "Robust head pose estimation using Dirichlet-tree distribution enhanced random forests", Neurocomputing, 2016, 173: 42-53. [<a href="pdf/journalpaper/Robust%20head%20pose%20estimation.pdf" target="_blank">pdf</a>]
        </td>
    </tr>
    <tr><td><br></td></tr>

    <tr>
        <td width="306"><img src="image/journalpaper/Head%20Pose%20Estimation.jpg" width="290px"></td>
        <td style="line-height: 120%">
            <b>Yuanyuan Liu</b>, Chen Jingying, Yu Kan, et al. "Head pose estimation based on tree-structure cascaded random forests in unconstrained environment", Journal of Electron and Information Technology, 2015, 37(3): 543-551. (in Chinese)[<a href="pdf/journalpaper/Head%20Pose%20Estimation.pdf" target="_blank">pdf</a>]
        </td>
    </tr>
    <tr><td><br></td></tr>

    <tr>
        <td width="306"><img src="image/journalpaper/A%20novel%20angle-tuned%20thin%20film%20filter.jpg" width="290px"></td>
        <td style="line-height: 120%">
            Kan Yu, <b>Yuanyuan Liu</b>, J Yin, et al. "A novel angle-tuned thin film filter with low angle sensitivity", Optics & Laser Technology, 2015, 68:141-145.[<a href="pdf/journalpaper/A%20novel%20angle-tuned%20thin%20film%20filter.pdf" target="_blank">pdf</a>]
        </td>
    </tr>
    <tr><td><br></td></tr>

    <tr>
        <td width="306"><img src="image/journalpaper/A%20Hierarchical%20Regression%20Approach.jpg" width="290px"></td>
        <td style="line-height: 120%">
            <b>Yuanyuan Liu</b>, Jingying Chen, Cunjie Shan, Zhiming Su, Pei Cai. "A Hierarchical Regression Approach for Unconstrained Face Analysis". International Journal of Pattern Recognition and Artificial Intelligence, 2015, Vol. 29, No. [<a href="pdf/journalpaper/A%20Hierarchical%20Regression%20Approach.pdf" target="_blank">pdf</a>]
        </td>
    </tr>
    <tr><td><br></td></tr>

    <!--    <tr>-->
    <!--        <td width="306"><img src="image/journalpaper/A%20hybrid%20intelligence-aided%20approach.jpg" width="290px"></td>-->
    <!--        <td>-->
    <!--            Jingying Chen, Nan Luo, <b>Yuanyuan Liu</b>, Leyuan Liu, Kun Zhang, Joanna Kolodziej, "A hybrid intelligence-aided approach to affect-sensitive e-learning", Computing, Springer, 2014. [<a href="pdf/journalpaper/A%20hybrid%20intelligence-aided%20approach.pdf" target="_blank">pdf</a>]-->
    <!--        </td>-->
    <!--    </tr>-->
    <!--    <tr><td><br></td></tr>-->

    <!--    <tr>-->
    <!--        <td width="306"><img src="image/journalpaper/Design%20of%20angle-tuned%20wedge%20narrowband%20thin%20film%20filter.jpg" width="290px"></td>-->
    <!--        <td>-->
    <!--            Kan Yu, <b>Yuanyuan Liu</b>, Jiaqi Bao, et al. "Design of angle-tuned wedge narrowband thin film filter", Optics & Laser Technology, 2014, 56(1):71–75.[<a href="pdf/journalpaper/Design%20of%20angle-tuned%20wedge%20narrowband%20thin%20film%20filter.pdf" target="_blank">pdf</a>]-->
    <!--        </td>-->
    <!--    </tr>-->
    <!--    <tr><td><br></td></tr>-->

    <!--    <tr><td><strong>2020</strong></td></tr>-->
    <!--    <tr><td><hr></td><td><hr></td></tr>-->
    <tr><td><br></td></tr>
    <tr><td><br></td></tr>
    <tr><td><strong>Conference Paper:</strong></td></tr>
    <tr><td><br></td></tr>

    <tr>
        <td width="306"><hr></td>
        <td><hr></td>
    </tr>
    <tr><td><br></td></tr>

    <tr>
        <td width="306"><img src="image/conferencepaper/ftp.png" width="290px"></td>
        <td style="line-height: 120%">
            <b>Yuanyuan Liu</b>, Yuxuan Huang, Shuyang Liu, Yibing Zhan, Zijing Chen, Zhe Chen. "Open-Set Video-based Facial Expression Recognition with Human Expression-sensitive Prompting", ACM International Conference on Multimedia (MM ’24). [<a href="pdf/conferencepaper/ftp4871 camera-ready.pdf" target="_blank">pdf</a>][<a href="https://github.com/cosinehuang/HESP" target="_blank">code </a>] [<a href="pdf/conferencepaper/ftp4871 Supplementary.pdf" target="_blank">supp</a>]
        </td>
    </tr>
    <tr><td><br></td></tr>

    <tr>
        <td width="306"><img src="image/conferencepaper/PCL.png" width="290px"></td>
        <td style="line-height: 120%">
            <b>Yuanyuan Liu</b>, Wenbin Wang, Yibing Zhan, Shaoze Feng, Kejun Liu, Zhe Chen. "Pose-disentangled Contrastive Learning for Self-supervised Facial Representation." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023. [<a href="pdf/conferencepaper/Liu_Pose-Disentangled_Contrastive_Learning_for_Self-Supervised_Facial_Representation_CVPR_2023_paper.pdf" target="_blank">pdf</a>][<a href="https://github.com/DreamMr/PCL" target="_blank">code </a>]
        </td>
    </tr>
    <tr><td><br></td></tr>
        
    <tr>
        <td width="306"><img src="image/conferencepaper/MAFW%20A%20Large-scale,%20Multi-modal,%20Compound%20Affective%20Database%20for%20Dynamic%20Facial%20Expression%20Recognition%20in%20the%20Wild.png" width="290px"></td>
        <td style="line-height: 120%">
            <b>Yuanyuan Liu</b>,  Wei Dai, Chuanxu Feng, Wenbin Wang, Guanghao Yin, Jiabei Zeng, Shiguang Shan. "MAFW: A Large-scale, Multi-modal, Compound Affective Database for Dynamic Facial Expression Recognition in the Wild", ACM International Conference on Multimedia (MM ’22 Oral). [<a href="pdf/conferencepaper/MAFW%20A%20Large-scale,%20Multi-modal,%20Compound%20Affective%20Database%20for%20Dynamic%20Facial%20Expression%20Recognition%20in%20the%20Wild.pdf" target="_blank">pdf</a>][<a href="https://mafw-database.github.io/MAFW/" target="_blank">MAFW Database URL </a>][<a href="pdf/conferencepaper/MAFW_supp.pdf" target="_blank">supp</a>]
        </td>
    </tr>
    <tr><td><br></td></tr>

    <tr>
        <td width="306"><img src="image/conferencepaper/HIERARCHICAL%20DOMAIN-CONSISTENT%20NETWORK.png" width="290px"></td>
        <td style="line-height: 120%">
            <b>Yuanyuan Liu</b>, Ziyang Liu, Fang Fang, ZHanghua Fu, Zhanlong Chen, “Hierarchical Domain-Consistent Network for Cross-Domain Object Detection”, IEEE International Conference on Image Processing(ICIP),2021. [<a href="pdf/conferencepaper/HIERARCHICAL%20DOMAIN-CONSISTENT%20NETWORK.pdf" target="_blank">pdf</a>]
        </td>
    </tr>
    <tr><td><br></td></tr>

    <tr>
        <td width="306"><img src="image/conferencepaper/Multi-Channel%20Pose-Aware%20Convolution%20Neural%20Networks.jpg" width="290px"></td>
        <td style="line-height: 120%">
            <b>Yuanyuan Liu</b>, Jiabei Zeng, Shiguang Shan, Zuo Zheng, "Multi-Channel Pose-Aware Convolution Neural Networks for Multi-View Facial Expression Recognition", proceeding of 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG), 2018: 458-464. [<a href="pdf/conferencepaper/Multi-Channel%20Pose-Aware%20Convolution%20Neural%20Networks.pdf" target="_blank">pdf</a>]
        </td>
    </tr>
    <tr><td><br></td></tr>

    <tr>
        <td width="306"><img src="image/conferencepaper/Deep%20Transfer%20Feature%20Based%20Convolutional%20Neural%20Forests.jpg" width="290px"></td>
        <td style="line-height: 120%">
            <b>Yuanyuan Liu</b>, Zhong Xie, Xi Gong, Fang Fang. "Deep Transfer Feature Based Convolutional Neural Forests for Head Pose Estimation", Pacific-Rim Symposium on Image and Video Technology (PRSIVT), Springer, Cham, 2017: 5-16. [<a href="pdf/conferencepaper/Deep%20Transfer%20Feature.pdf" target="_blank">pdf</a>]
        </td>
    </tr>
    <tr><td><br></td></tr>

    <tr>
        <td width="306"><img src="image/conferencepaper/Urban%20function%20zoning.jpg" width="290px"></td>
        <td style="line-height: 120%">
            Lu Wang, Fang Fang, Xiaohui Yuan, <b>Yuanyuan Liu</b>, Zhongwen Luo, "Urban function zoning using geotagged photos and open street map", IEEE Geoscience and Remote Sensing Symposium (IGRSS), 2017: 815-818. [<a href="pdf/conferencepaper/Urban%20Function%20Zoning.pdf" target="_blank">pdf</a>]
        </td>
    </tr>
    <tr><td><br></td></tr>

    <tr>
        <td width="306"><img src="image/conferencepaper/A%20Performance%20Evaluation%20Model.jpg" width="290px"></td>
        <td style="line-height: 120%">
            Huiming Lv, Fang Fang, Yishi Zhao, <b>Yuanyuan Liu</b>, Zhongwen Luo,"A Performance Evaluation Model for Taxi Cruising Path Recommendation System",Pacific-Asia Conference on Knowledge Discovery and Data Mining (PACKDD) , 2017, 156-167.[<a href="pdf/conferencepaper/A%20Performance%20Evaluation%20Model.pdf" target="_blank">pdf</a>]
        </td>
    </tr>
    <tr><td><br></td></tr>

    <tr>
        <td width="306"><img src="image/conferencepaper/An%20Intelligent%20Learning%20System.jpg" width="290px"></td>
        <td style="line-height: 120%">
            <b>Yuanyuan Liu</b>, Zhong Xie, Jingying Chen. "An Intelligent Learning System for Supporting Interactive Learning through Student Engagement Study", 12th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD), 2016: 498-503.[<a href="pdf/conferencepaper/An%20Intelligent%20Learning%20System.pdf" target="_blank">pdf</a>]
        </td>
    </tr>
    <tr><td><br></td></tr>

    <!--    <tr>-->
    <!--        <td width="306"><img src="image/conferencepaper/Dirichlet-tree%20Distribution%20Enhanced%20Random%20Forests.jpg" width="290px"></td>-->
    <!--        <td>-->
    <!--            <b>Yuanyuan Liu</b>, Jingying Chen, Cunjie Shan. "Dirichlet-tree Distribution Enhanced Random Forests for facial feature detection", IEEE International Conference on Image Processing (ICIP), 2014: 234-238. [<a href="pdf/conferencepaper/DIRICHLET-TREE%20DISTRIBUTION%20ENHANCED%20RANDOM%20FORESTS.pdf" target="_blank">pdf</a>]-->
    <!--        </td>-->
    <!--    </tr>-->
    <!--    <tr><td><br></td></tr>-->

    <!--    <tr>-->
    <!--        <td width="306"><img src="image/conferencepaper/Dirichlet-tree%20Cascaded%20Hough%20forests.jpg" width="290px"></td>-->
    <!--        <td>-->
    <!--            <b>Yuanyuan Liu</b>, Jingying Chen, Haiqing Chen, "Dirichlet-tree Cascaded Hough forests for Continuous Head Pose Estimation", The IEEE The 7th International Congress on Image and Signal Processing (CISP), 2014: 554-559. [<a href="pdf/conferencepaper/Dirichlet-tree%20Cascaded%20Hough%20forests.pdf" target="_blank">pdf</a>]-->
    <!--        </td>-->
    <!--    </tr>-->
    <!--    <tr><td><br></td></tr>-->

    <!--    <tr>-->
    <!--        <td width="306"><img src="image/conferencepaper/Dirichlet-tree%20Distribution%20Enhanced%20Random%20Forests(2).jpg" width="290px"></td>-->
    <!--        <td>-->
    <!--            <b>Yuanyuan Liu</b>, Jingying Chen, Leyuan Liu, Yujiao Gong, Nan Luo. "Dirichlet-tree Distribution Enhanced Random Forests for Head Pose Estimation", International Conference on Pattern Recognition Applications and Methods (ICPRAM), 2014: 87-95. [<a href="pdf/conferencepaper/Dirichlet-tree%20Distribution%20Enhanced%20Random%20Forests%20(2).pdf" target="_blank">pdf</a>]-->
    <!--        </td>-->
    <!--    </tr>-->
    <!--    <tr><td><br></td></tr>-->



    </tbody></table>
<br>
<hr>

<h3>Research Project (PI)</h3>
<p>
<ol id="ProjectsOL" style="line-height: 150%">
    <li>Jan.2021~Dec.2024, “Research on Video Emotion Detection and Recognition Method under Unconstrained Conditions”, Sponsored by National Science Foundation (No. 62076227), PI</li>
    <li>Sep.2020~Dec.2023, “Key technologies and applications of video-based intelligent emotional computing”, Sponsored by Wuhan Basic Frontier Project of Science and Technology , PI</li>
    <li>Sep.2019~Dec.2020, “Intelligent rapid recognition technology of typical targets based on few-shot learning”, Sponsored by Aviation Science and Industry Joint Fund Project, PI</li>
    <li>Jan.2016~Dec.2019, “Research on spontaneous expression recognition based on deep enhanced random forests under multi-noises”, Sponsored by National Science Foundation (No. 61602429), PI</li>
    <li>Jan.2016~Dec.2018, “Research on spontaneous facial expression recognition under multi pose and occlusion conditions”, Supported by China Postdoctoral Science Foundation. (No. 2016M592406), PI</li>
    <li>Jan.2016~Jun.2018, “Research on human behavior recognition, tracking and prediction in infrared videos”, Development Projects, Sponsored by the Fundamental Research Funds for the Central Universities, China University of Geosciences (Wuhan) (No. 26420160055), PI</li>
</ol>
</p>

<hr>



<h3>PROFESSIONAL ACTIVITIES:</h3>
<p><em>Paper reviewer</em></p>
<table>
    <tbody>
    <tr>
        <td>Pattern recognition (PR)</td>
    </tr>
    <tr>
        <td>Multimedia and tools application (MTA)</td>
    </tr>
    <tr>
        <td>IEEE Transactions on Automation Science and Engineering (TASE)</td>
    </tr>

    <tr>
        <td>International Journal of Remote Sensing (IJRS)</td>
    </tr>

    <tr>
        <td>IEEE Transactions on Geosciences and Remote Sensing (TGRS)</td>
    </tr>
    <tr>
        <td>Frontiers of Computer Science(FCS)</td>
    </tr>
    <tr>
        <td>IEEE Access</td>
    </tr>
    </tbody></table>
<p><em>Social work</em></p>
<table>
    <tbody>
    <tr>
        <td>Visiting Associate Researcher
        </td>
    </tr>
    <tr>
        <td>Institute of Robotics and Artificial Intelligence
        </td>
    </tr>
    <tr>
        <td> The Chinese University of Hong Kong (Shenzhen)
        </td>
    </tr>

    </tbody>
</table>
<!--
<h3>FRIENDS:</h3>
<p><em><a href="https://sites.google.com/site/pichaossites/">Pichao Wang</a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <a href="http://www.escience.cn/people/liuxinwang">Xinwang Liu</a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <a href="http://cs.tju.edu.cn/faculty/zhangchangqing/">Changqing Zhang</a></em></p>
<p></p>
-->

<hr>
<h3>Team Member</h3>
<table width="100%">
    <tbody>
    <tr><td><br></td></tr>
    <tr align="center">
        <!--        <td width="16.6%"><a href=""><img src="image/member/jinxiangguo.jpg" width="100%">Jin Xiangguo</a></td>-->
        <td width="16.6%"><a href=""><img src="image/member/fengshaoze.jpg" width="100%">Feng Shaoze</a></td>
        <td width="16.6%"><a href=""><img src="image/member/huangyuxuan.jpg" width="100%">Huang Yuxuan</a></td>
        <td width="16.6%"><a href=""><img src="image/member/weilin.jpg" width="100%">Wei Lin</a></td>
        <td width="16.6%"><a href=""><img src="image/member/liukejun.jpg" width="100%">Liu Kejun</a></td>
        <td width="16.6%"><a href=""><img src="image/member/lsy.jpg" width="100%">Liu Shuyang</a></td>
        <td width="16.6%"><a href=""><img src="image/member/wk.jpg" width="100%">Wang Ke</a></td>
    </tr>
    <tr><td><br></td></tr>
    <tr align="center">
        <!--        <td width="16.6%"><a href=""><img src="image/member/jinxiangguo.jpg" width="100%">Jin Xiangguo</a></td>-->
        <td width="16.6%"><a href=""><img src="image/member/lyj.jpg" width="100%">Liu Yunjiao</a></td>
        <td width="16.6%"><a href=""><img src="image/member/wyc.jpg" width="100%">Yuan Yuchen</a></td>
        <td width="16.6%"><a href=""><img src="image/member/qianying.jpg" width="100%">Qian Ying</td>
        <td width="16.6%"><a href=""><img src="image/member/xulei.png" width="100%">Xu Lei</td>
        <td width="16.6%"><a href=""><img src="image/member/zhouqiyin.jpg" width="100%">Zhou QiYin</td>
        <td width="16.6%"><a href=""><img src="image/member/pengjiyao.jpg" width="100%">Peng Jiyao（guaduated）</a></td> 
    </tr>
    <tr><td><br></td></tr>
    <tr align="center">
        <td width="16.6%"><a href=""><img src="image/member/daiwei.png" width="100%">Dai Wei（guaduated）</a></td>
        <td width="16.6%"><a href=""><img src="image/member/wangyu.jpg" width="100%">Wang Yu（guaduated）</a></td>
        <td width="16.6%"><a href=""><img src="image/member/liuziyang.png" width="100%">Liu Ziyang（guaduated）</a></td>
        <td width="16.6%"><a href=""><img src="image/member/wangkunpeng.jpg" width="100%">Wang Kunpeng（guaduated）</a></td>
        <td width="16.6%"><a href=""><img src="image/member/fengchuanxu.png" width="100%">Feng Chuanxu（guaduated）</a></td>
        <td width="16.6%"><a href=""><img src="image/member/wangwenbin.jpg" width="100%">Wang Wenbin（guaduated）</a></td>

    </tr>
    <tr><td><br></td></tr>
    <tr align="center">
        <td width="16.6%"><a href=""><img src="image/member/zhanghaoyu.jpg" width="100%">Zhang Haoyu（guaduated）</a></td>
        <td width="16.6%"><a href=""><img src="image/member/wangchaofan.jpg" width="100%">Wang Chaofan（guaduated）</a></td>
        <td width="16.6%"><a href=""><img src="image/member/zhouning.jpg" width="100%">Zhou Ning（guaduated）</a></td>
        <td width="16.6%"><a href=""><img src="image/member/yinguanghao.jpg" width="100%">Yin Guanghao（guaduated）</a></td>
        <td width="16.6%"><a href=""><img src="image/member/lkl.jpg" width="100%">Li Kanglin（guaduated）</a></td>
        <td width="16.6%"><a href=""><img src="image/member/zhaoyaowu.jpg" width="100%">Zhao Yaowu（guaduated）</a></td>
    </tr>
        
    </tbody>
</table>

<hr>
<br>
<h4>WEBSITE VISITING STATISTICS</h4>
<table>
    <tbody>
    <tr>
        <td>
            <a href="https://clustrmaps.com/site/1birr"  title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=N2t4JJQU4CTbbZSYoeGo7EdtfN81cu4m3ki6BB3Fc68&cl=ffffff" /></a>
        </td>
    </tr>
    </tbody>
</table>
<br>

<!-- <textarea name="" id="writeArea"></textarea>
<button>发布</button>
<ul>

</ul>

<script>
    var text = document.querySelector("textarea");
    var btn = document.querySelector("button");
    var ul = document.querySelector("ul");

    btn.onclick = function () {
    //   获取textarea的值
        var val = text.value;
        // 如果值为空不添加
        if (val == "") {
          alert("请先输入内容");
          //   函数里面终止用return，break在函数外用
          return;
        }
        // 创建一个li
        var li = document.createElement("li");
        // 让li的值等于textarea的值
        li.innerText = val;
        // 把li增加到ul里面,插入到第一行
        ul.insertBefore(li, ul.children[0]);
        // 添加内容之后内容清空
        text.value = "";
    };

</script> -->


<script>
    function displayTime(){
        var elt = document.getElementById("clock");
        var now = new Date();
        elt.innerHTML  = now.toLocaleDateString() + " " + now.toLocaleTimeString();
        setTimeout(displayTime,1000);
        }
    window.onload = displayTime;
</script>
<style>
#clock{
    font: bold 15pt sans;
    background: rgba(238, 237, 240, 0.507);
    padding: 10px;
    border: solid rgb(170, 170, 170) 2px;
    border-radius: 10px;
    display: inline-block;
    margin: 0 auto;}
</style>


</body></html>
