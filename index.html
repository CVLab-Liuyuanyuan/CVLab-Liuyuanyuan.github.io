<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="">
<meta name="author" content="">
<link href="https://fonts.useso.com/css?family=Lato:100,300,400,700,900" rel="stylesheet">
<title>Home</title>
<!-- Bootstrap core CSS -->
<link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

<!-- Additional CSS Files -->
<link rel="stylesheet" href="assets/css/fontawesome.css">
<link rel="stylesheet" href="assets/css/templatemo-style.css">
<link rel="stylesheet" href="assets/css/owl.css">
<link rel="stylesheet" href="assets/css/lightbox.css">

</head>
<body>

<div id="page-wraper">

<!--header-->
<header class="main-header clearfix" role="header">
  <div class="container">
	<div class="image">
	  <img src="assets/images/author-image.jpg" alt="">
	</div>
    <br>
	<nav class="main-nav" role="navigation">
	  <ul class="main-menu">
		<li><a href="#Profile">Profile</a></li>
		<li><a href="#Publications">Publications</a></li>
		<li><a href="#Patent">Patent</a></li>
		<li><a href="#Students">Students</a></li>
        <li><a href="#Projects">Our Source</a></li>
	  </ul>
	</nav>
	<div class="social-network">
	  <ul class="soial-icons">
		<li><a href="#"><i class="fa fa-facebook"></i></a></li>
		<li><a href="#"><i class="fa fa-twitter"></i></a></li>
		<li><a href="#"><i class="fa fa-linkedin"></i></a></li>
		<li><a href="#"><i class="fa fa-dribbble"></i></a></li>
		<li><a href="#"><i class="fa fa-rss"></i></a></li>
	  </ul>
	</div>

  </div>
</header>

<!-- Sidebar Menu -->
<div class="responsive-nav">
  <i class="fa fa-bars" id="menu-toggle"></i>
  <div id="menu" class="menu">
	<i class="fa fa-times" id="menu-close"></i>
	<div class="container">
	  <div class="image">
		<img src="assets/images/author-image.jpg" alt="">
	  </div>
	  <div class="author-content">
		<h4>Reflux Me</h4>
		<span>Web Designer</span>
	  </div>
	  <nav class="main-nav" role="navigation">
		<ul class="main-menu">
		  <li><a href="#Profile">Profile</a></li>
		  <li><a href="#Publications">Publications</a></li>
		  <li><a href="#Patent">Member_Info</a></li>
		  <li><a href="#Students">Students</a></li>
          <li><a href="#Projects">Our Source</a></li>
		</ul>
	  </nav>
	  <div class="social-network">
		<ul class="soial-icons">
		  <li><a href="#"><i class="fa fa-facebook"></i></a></li>
		  <li><a href="#"><i class="fa fa-twitter"></i></a></li>
		  <li><a href="#"><i class="fa fa-linkedin"></i></a></li>
		  <li><a href="#"><i class="fa fa-dribbble"></i></a></li>
		  <li><a href="#"><i class="fa fa-rss"></i></a></li>
		</ul>
	  </div>
	  <div class="copyright-text">
		<p>Copyright 2019 Reflux Design(meant to be deleted..)</p>
	  </div>
	</div>
  </div>
</div>

<section class="section about-me" data-section="Profile">
  <div class="container">
	<div class="section-heading">
	  <div class="line-dec"></div>
	  <div style="width: 100%;display: flex">
        <div style="width: 30%"><a href="http://grzy.cug.edu.cn/liuyuanyuan/zh_CN/index.htm" target="_blank"><img src="assets/member_img/Liu_Yuanyuan.jpg" /></a></div>
        <div style="width: 5%"></div>
        <div style="width: 65%" align="left">
            <a href="http://grzy.cug.edu.cn/liuyuanyuan/zh_CN/index.htm" target="_blank"><h1 style="color: #F39100">Liu Yuanyuan</h1></a>
            <h5 style="font: oblique;color: #F39100">Associate Professor</h5>
            <h5>School of Information and Engineering, 
             University of Geosciences (Wuhan), Hubei, 
             430074, China</h5>
            <span style="font: oblique;color: #F39100">Telephone:</span><span>86-013349830890</span>
            <br>
            <span style="font: oblique;color: #F39100">Email:</span><span>liuyy@cug.edu.cn</span>
        </div>
      </div>
      <br><br>
      <div style="width: 100%" align="left">
          <h4 style="color: aliceblue">Research Area<br></h4>
          <h5>The Comper vision and emotion computing lab (CVEC-Lab) focuses on the following topics:</h5>
          <h5>&#8226&nbspFace analysis including head pose estimation, facial expression recognition, face detection, and facial feature localization in unconstrained environment</h5>
          <h5>&#8226&nbspScene recognition and understanding including scene recognition, object detection, and semantic segmentation</h5>
          <h5>&#8226&nbspHuman computer interface including engagement study and multi-person visual focus of attention recognition</h5>
      </div>
      <br>
      <div style="width: 100%" align="left">
          <h4 style="color: aliceblue">Professional Activites<br></h4>
          <h5><b><i>Paper reviewer :</i></b> Pattern recognition (PR)、Multimedia and tools application (MTA)、IEEE Transactions on Automation Science and Engineering (TASE)、International Journal of Remote Sensing (IJRS)、IEEE Transactions on Geosciences and Remote Sensing (TGRS)、Frontiers of Computer Science（FCS）、IEEE Access、etc.</h5>
          <h5><b><i>Social work :</i></b> Visiting Associate Researcher, Institute of Robotics and Artificial Intelligence, The Chinese University of Hong Kong (Shenzhen)</h5>
      </div>
      <br>
      <div style="width: 100%" align="left">
          <h4 style="color: aliceblue">Research Project (PI)<br></h4>
          <h5>&#8226&nbspJan.2021~Dec.2024, “Research on Video Emotion Detection and Recognition Method under Unconstrained Conditions”, Sponsored by National Science Foundation (No. 62076227), PI</h5>
          <h5>&#8226&nbspSep.2020~Dec.2023, “Key technologies and applications of video-based intelligent emotional computing”, Sponsored by Wuhan Basic Frontier Project of Science and Technology , PI</h5>
          <h5>&#8226&nbspSep.2019~Dec.2020, “Intelligent rapid recognition technology of typical targets based on few-shot learning”, Sponsored by Aviation Science and Industry Joint Fund Project, PI</h5>
          <h5>&#8226&nbspJan.2016~Dec.2019, “Research on spontaneous expression recognition based on deep enhanced random forests under multi-noises”, Sponsored by National Science Foundation (No. 61602429), PI</h5>
          <h5>&#8226&nbspJan.2016~Dec.2018, “Research on spontaneous facial expression recognition under multi pose and occlusion conditions”, Supported by China Postdoctoral Science Foundation. (No. 2016M592406), PI</h5>
          <h5>&#8226&nbspJan.2016~Jun.2018, “Research on human behavior recognition, tracking and prediction in infrared videos”, Development Projects, Sponsored by the Fundamental Research Funds for the Central Universities, China University of Geosciences (Wuhan) (No. 26420160055), PI</h5>
      </div>
	</div>
  </div>
</section>

<section class="section my-services" data-section="Publications">
  <div class="container">
	<div class="section-heading">
	  <h2>Publications</h2>
	  <div class="line-dec"></div>
	  <br>
      <div style="width: 100%" align="left">
          <h4 style="color: aliceblue">Journal Paper</h4>
          <h5>&#8226&nbsp<b><i>Yuanyuan Liu</i></b>, Jiyao Peng, Jiabei Zeng, Shiguang Shan, “Joint Spatial and Scale Attention Network for Multi-view Facial Expression Recognition”, IEEE Transaction on Image Processing (TIP), 2020. (Under review).</h5>
          <h5>&#8226&nbsp<b><i>Yuanyuan Liu</i></b>, Dingyuan Chen, Ailong Ma, Yanfei Zhong, Fang Fang, Kai Xu “A Multi-Scale U-Shaped CNN Building Instance Extraction Framework With Edge Constraint for High Spatial Resolution Remote Sensing Imagery”, IEEE Transaction on Geosciences and Remote Sensing (TGRS), 2020, 10.1109/TGRS.2020.3022410.</h5>
          <h5>&#8226&nbsp<b><i>Yuanyuan Liu</i></b>, Xiaohui Yuan, Xi Gong, Zhong Xie, Fang Fang, Zhongwen Luo,  “Conditional convolution neural network enhanced random forest for facial expression recognition”, Pattern Recognition, Volume 84, 2018, Pages 251-261.</h5>
          <h5>&#8226&nbsp<b><i>Yuanyuan Liu</i></b>, Xinmei Li, Fayong Zhang, Fang Fang, Jingying Chen, Zhizhong Zeng. “Visual Focus of Attention and Spontaneous Smile Recognition based on Continuous Head Pose Estimation by Cascaded Multi-task Learning”, International Journal of Pattern Recognition and Artificial Intelligence, 2018, https://doi.org/10.1142/S0218001419400068.</h5>
          <h5>&#8226&nbsp<b><i>Yuanyuan Liu</i></b>, Zhong Xie, Xiaohui Yuan, Jingying Chen, Wu Song. “Multi-level Structured Hybrid Forest for Joint Head Detection and Pose Estimation”. Neurocomputing, 266(11), 206-215 2017.</h5>
          <h5>&#8226&nbsp<b><i>Yuanyuan Liu</i></b>, Jingying Chen, Zhiming Su, et al., “Robust head pose estimation using Dirichlet-tree distribution enhanced random forests”, Neurocomputing, 2016, 173: 42-53. </h5>
          <h5>&#8226&nbsp<b><i>Yuanyuan Liu</i></b>, Jingying Chen, Mulan Zhang, Chuan Rao. “Student Engagement Study based on Multi-cue detection and recognition in an Intelligent Learning Environment”, Multimedia tools and applications, 2018, 1-27.</h5>
          <h5>&#8226&nbsp<b><i>Yuanyuan Liu</i></b>, Jingying Chen, Cunjie Shan, Zhiming Su, Pei Cai. “A Hierarchical Regression Approach for Unconstrained Face Analysis”. International Journal of Pattern Recognition and Artificial Intelligence, 2015, Vol. 29, No. 8.</h5>
          <h5>&#8226&nbspXi Gong, Zhong Xie, <b><i>Yuanyuan Liu*</i></b>, Zhuo ZHENG. “Deep Salient Feature based Transfer Convolutional Neural Network for Scene Classification of Remote Sensing Imagery”, Remote Sensing 2018, 10(3), 410. </h5>
          <h5>&#8226&nbspZhuo Zheng, Fang Fang,<b><i>Yuanyuan Liu*</i></b> , Gong Xi, Guo Mingqiang, Luo Zhongwen. “Joint Multi-scale Convolution Neural Network for Scene Classification of High Resolution Remote Sensing Imagery”. Acta Geodaetica et Cartographica Sinica, 2018, 47(5): 620-630. (in Chinese)</h5>
          <h5>&#8226&nbsp<b><i>Yuanyuan Liu</i></b>, Zhong Xie, Shunping Zhou, Zheng Liu, Weiming Wang, Xiuping Liu, Wei Rao. “Conditional Iteration Updated Random Forests for Unconstrained Facial Feature Location”, Journal of computer aided design and graphics, Vol.29, No.10, 1881-1890 , 2017. (in Chinese)</h5>
          <h5>&#8226&nbsp<b><i>Yuanyuan Liu</i></b>, Chen Jingying, Yu Kan, et al. “Head pose estimation based on tree-structure cascaded random forests in unconstrained environment”, Journal of Electron and Information Technology, 2015, 37(3): 543-551. (in Chinese)</h5>
          <h5>&#8226&nbspKan Yu,<b><i>Yuanyuan Liu</i></b> , Jiaqi Bao, et al. “Design of angle-tuned wedge narrowband thin film filter”, Optics & Laser Technology, 2014, 56(1):71–75.</h5>
          <h5>&#8226&nbspKan Yu,<b><i>Yuanyuan Liu</i></b> , J Yin, et al. “A novel angle-tuned thin film filter with low angle sensitivity”, Optics & Laser Technology, 2015, 68:141–145.</h5>
          <h5>&#8226&nbspJingying Chen, Nan Luo, <b><i>Yuanyuan Liu</i></b>, Leyuan Liu, Kun Zhang, Joanna Kolodziej, “A hybrid intelligence-aided approach to affect-sensitive e-learning”, Computing, Springer, 2014. </h5>
      </div>
      <br>
      <div style="width: 100%" align="left">
          <h4 style="color: aliceblue">Conference Paper</h4>
          <h5>&#8226&nbsp <b><i>Yuanyuan Liu</i></b>, Jiabei. Zeng, Shiguang Shan, Zuo Zheng, “Multi-Channel Pose-Aware Convolution Neural Networks for Multi-View Facial Expression Recognition”, proceeding of 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG), 2018: 458-464. </h5>
          <h5>&#8226&nbsp <b><i>Yuanyuan Liu</i></b>, Jingying Chen, Cunjie Shan. “Dirichlet-tree Distribution Enhanced Random Forests for facial feature detection”, IEEE International Conference on Image Processing (ICIP), 2014: 234-238. </h5>
          <h5>&#8226&nbsp <b><i>Yuanyuan Liu</i></b>, Leyuan Liu, Jingying Chen, et al. “Multi-person Visual Focus of Attention from Head Pose on a Natural Classroom”, International Conference on Pattern Recognition Applications and Methods (ICPRAM), 2016: 205-213. </h5>
          <h5>&#8226&nbsp <b><i>Yuanyuan Liu</i></b>, Jingying Chen, Leyuan Liu, Yujiao Gong, Nan Luo. “Dirichlet-tree Distribution Enhanced Random Forests for Head Pose Estimation”, International Conference on Pattern Recognition Applications and Methods (ICPRAM), 2014: 87-95. </h5>
          <h5>&#8226&nbsp <b><i>Yuanyuan Liu</i></b>, Jingying Chen, Haiqing Chen, “Dirichlet-tree Cascaded Hough forests for Continuous Head Pose Estimation”, The IEEE The 7th International Congress on Image and Signal Processing (CISP), 2014: 554-559. </h5>
          <h5>&#8226&nbsp <b><i>Yuanyuan Liu</i></b>, Zhong Xie, Jingying Chen. “An Intelligent Learning System for Supporting Interactive Learning through Student Engagement Study”, 12th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD), 2016: 498-503.</h5>
          <h5>&#8226&nbsp <b><i>Yuanyuan Liu</i></b>, Zhong Xie, Xi Gong, Fang Fang. “Deep Transfer Feature Based Convolutional Neural Forests for Head Pose Estimation”, Pacific-Rim Symposium on Image and Video Technology (PRSIVT), Springer, Cham, 2017: 5-16.  </h5>
          <h5>&#8226&nbsp Lu Wang, Fang Fang, Xiaohui Yuan,<b><i>Yuanyuan Liu</i></b> , Zhongwen Luo, “Urban function zoning using geotagged photos and open street map”, IEEE Geoscience and Remote Sensing Symposium (IGRSS), 2017: 815-818. </h5>
          <h5>&#8226&nbsp Huiming Lv, Fang Fang, Yishi Zhao, <b><i>Yuanyuan Liu</i></b>, Zhongwen Luo,“A Performance Evaluation Model for Taxi Cruising Path Recommendation System”，Pacific-Asia Conference on Knowledge Discovery and Data Mining (PACKDD) , 2017, 156-167.</h5>
          <br>
      </div>

	</div>
  </div>
</section>

<section class="section my-work" data-section="Patent">
  <div class="container">
	<div class="section-heading">
	  <h2>Patent</h2>
	  <div class="line-dec"></div>
      <br>
      <div style="width: 100%" align="left">
        <h5>&#8226&nbsp 2017, “A method of scene classification for high resolution remote sensing images in limited available datasets”, Patent No. CN107220657B</h5>
        <h5>&#8226&nbsp 2016, “A method and system for facial feature localization in unconstrained environment”, Patent No. CN106529397B</h5>
        <h5>&#8226&nbsp 2016, “Immersive interaction method for spatial data based on eye tracking”, Patent No. CN106774950A</h5>
    </div>
	</div>
  </div>
</section>    
    
<section class="section my-work" data-section="Students">
  <div class="container">
	<div class="section-heading">
	  <h2>Students</h2>
	  <div class="line-dec"></div>
      <br>
      <h4 style="color: aliceblue" align="left">Ph.D. Students</h4>
      <table style="table-layout: fixed;" width="100%" cellspacing="10" cellpadding="20">
        <tr>
            <td align="center">
                <a href="#" style="color: white"><img src="assets/member_img/Wang_Yu.jpg"  class="img-fluid rounded-circle" />Wang Yu</a>
            </td>
            <td align="center">
            </td>
            <td align="center">
            </td>
            <td align="center">
            </td>
        </tr>  
    </table>
      <h4 style="color: aliceblue" align="left">Master Students</h4>
      <table style="table-layout: fixed;" width="100%" cellspacing="10" cellpadding="20">
        <tr>
            
            <td align="center">
                <a href="#" style="color: white"><img src="assets/member_img/Wang_Kunpeng.jpg"  class="img-fluid rounded-circle" />Wang Kunpeng</a>
            </td>
            <td align="center">
                <a href="#" style="color: white"><img src="assets/member_img/Wang_Wenbin.jpg"  class="img-fluid rounded-circle" />Wang Wenbin</a>
            </td>
            <td align="center">
                <a href="#" style="color: white"><img src="assets/member_img/Peng_Jiyao.jpg"  class="img-fluid rounded-circle" />Peng Jiyao</a>
            </td>
            <td align="center">
            </td>
        </tr>  
    </table>
	</div>
     
  </div>
</section>

<!--<section class="section contact-me" data-section="Projects">-->
<!--  <div class="container">-->
<!--	<div class="section-heading">-->
<!--	  <h2>Projects</h2>-->
<!--	  <div class="line-dec"></div>-->
<!--	  <span>To be finished...</span>-->
<!--	</div>-->
<!--    <div class="left-image-post">-->
<!--	  <div class="row">      -->
<!--		<div class="col-md-6">-->
<!--		  <div class="left-image">-->
<!--			<img src="assets/images/left-image.jpg" alt="">-->
<!--		  </div>-->
<!--		</div>-->
<!--		<div class="col-md-6">-->
<!--		  <div class="right-text">-->
<!--			<h4>项目一</h4>-->
<!--                <p style="word-break: break-wrod;">To be finished...</p>-->
<!--			<div class="white-button">-->
<!--			  <a href="#">了解更多</a>-->
<!--			</div>-->
<!--		  </div>-->
<!--		</div>-->
<!--	  </div>-->
<!--  </div>-->
<!--  <br>-->
<!--  <div class="left-image-post">-->
<!--	  <div class="row">      -->
<!--		<div class="col-md-6">-->
<!--		  <div class="left-image">-->
<!--			<img src="assets/images/left-image.jpg" alt="">-->
<!--		  </div>-->
<!--		</div>-->
<!--		<div class="col-md-6">-->
<!--		  <div class="right-text">-->
<!--			<h4>项目二</h4>-->
<!--			<p>待补充...</p>-->
<!--			<div class="white-button">-->
<!--			  <a href="#">了解更多</a>-->
<!--			</div>-->
<!--		  </div>-->
<!--		</div>-->
<!--	  </div>-->
<!--  </div>-->
<!--</section>-->

    <section class="section contact-me" data-section="Projects">
      <div class="container">
    	<div class="section-heading">
    	  <h2>Our Source</h2>
    	  <div class="line-dec"></div><br>
    	  <a href="paper.html" target="_blank"><h4 style="color: yellow">Team Papers</h4></a>
    	</div>

      </div>
      <br>

    </section>


</div>



<!-- Scripts -->
<!-- Bootstrap core JavaScript -->
<script src="vendor/jquery/jquery.min.js"></script>
<script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

<script src="assets/js/isotope.min.js"></script>

<script src="assets/js/owl-carousel.js"></script>
<script src="assets/js/lightbox.js"></script>
<script src="assets/js/custom.js"></script>
<script>
	//according to loftblog tut
	$('.main-menu li:first').addClass('active');

	var showSection = function showSection(section, isAnimate) {
	  var
	  direction = section.replace(/#/, ''),
	  reqSection = $('.section').filter('[data-section="' + direction + '"]'),
	  reqSectionPos = reqSection.offset().top - 0;

	  if (isAnimate) {
		$('body, html').animate({
		  scrollTop: reqSectionPos },
		800);
	  } else {
		$('body, html').scrollTop(reqSectionPos);
	  }

	};

	var checkSection = function checkSection() {
	  $('.section').each(function () {
		var
		$this = $(this),
		topEdge = $this.offset().top - 80,
		bottomEdge = topEdge + $this.height(),
		wScroll = $(window).scrollTop();
		if (topEdge < wScroll && bottomEdge > wScroll) {
		  var
		  currentId = $this.data('section'),
		  reqLink = $('a').filter('[href*=\\#' + currentId + ']');
		  reqLink.closest('li').addClass('active').
		  siblings().removeClass('active');
		}
	  });
	};

	$('.main-menu').on('click', 'a', function (e) {
	  e.preventDefault();
	  showSection($(this).attr('href'), true);
	});

	$(window).scroll(function () {
	  checkSection();
	});
</script>

</body>
</html>