
<html xmlns="http://www.w3.org/1999/html"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <meta name="keywords" content="刘袁缘, 中国地质大学（武汉）, Yuanyuan Liu, machine learning, CUG, China University of Geosciences">
    <meta name="description" content="Liu Yuanyuan&#39;s home page">
    <link href="main_style.css" media="all" rel="stylesheet">
    <title>Liu Yuanyuan  - Homepage</title>
</head>

<body>
<!--<div id="navbar" align=right>-->
<!--    <a href="/">HOME</a>-->
<!--    <a href="publications.html">PUBLICATIONS</a>-->
<!--    <a href="teaching.html">TEACHING</a>-->
<!--    <a href="readinglist.html">READING</a>-->
<!--    <a href="conf.html">RESOURCES</a>-->
<!--</div>-->
<body>
    <span id="clock"></span>
</body>
<hr>
<br><br>
<table id="tbInformation" width="100%">
    <tbody>
    <tr>
        <td width="500">
            <!--            <h1>YUANYUAN LIU <img src="indexfiles/blank.png"></h1>-->
            <a href="http://grzy.cug.edu.cn/liuyuanyuan/zh_CN/index.htm" target="_blank"><h1>YUANYUAN LIU <img src="indexfiles/blank.png"></h1></a>
            <!--            <h1><img src="indexfiles/blank.png"></h1>-->
        </td>
        <td rowspan="3" align="center">
            <a href="http://grzy.cug.edu.cn/liuyuanyuan/zh_CN/index.htm" target="_blank"><img src="indexfiles/liuyuanyuan.png" border="0" width="300" height="300"></a>
        </td>
    </tr>


    <tr>
        <td>
            <h3>Professor</br>
                School of Information and Engineering</br>
                China University of Geosciences (Wuhan)</br>
                IEEE/CSIG/CAAI/ACM Member</br>
                CCF Senior Member</br>
                <p>No. <font face="Times New Roman">68</font> Jincheng Road,Wuhan, P.R. China Postcode: <font face="Times New Roman">430074.</font></p>
                <p><a href="https://scholar.google.com/citations?hl=en&user=o8hLiIcAAAAJ">Google Scholar</a>&nbsp;&nbsp;<a href="https://dblp.org/pid/97/2119-4.html">dblp</a></p>
                <strong>Telephone:<font face="Times New Roman"> 86-013349830890</font></br></strong>
                <strong>Email:</strong> liuyy@cug.edu.cn</br></h3>
        </td>
    </tr>

    </tbody>
</table>
<!--<table width="100%">
    <tbody>
    <tr>
        <td><font color="#0000FF"><strong>I am looking for new MS/BS students to join my research project working on machine learning and computer vision. If you have a <font color="#FF0000">good personality & solid mathematical & English background and happen to be interested in my research</font>, please contact me!</strong></font>
        </td>
    </tr>
    </tbody>
</table>-->

<hr>


<h3>Research Area</h3>
<h4>My research interests fall into the areas of <font color="red">computer vision, emotion computing, image and video understanding,</font> etc. More specifically, I am interested in the following topics:</h4>

<ol id="ProjectsList" style="line-height: 120%">
    <!-- <li>Head pose estimation, facial expression recognition, facial AU detection, and facial feature localization in unconstrained environment</li>
    <li>Human computer interface including emotion computing, group emotion recognition, and multi-model emotion recognition</li>
    <li>Image/Video understanding including scene recognition, object detection, and action recognition</li> -->
    <li><b>Emotion  Computing</b> including facial expression recognition, human-centric emotion analysis, emotion captioning, multimodal emotion recognition</li>
    <li><b>Image/Video understanding</b> including scene segmentation, cross-domain object detection, and temporal action localization</li>
    <li><b>Deep Learning Methods and Application</b> including open-set learning, zero-shot/few-shot learning, and domain adaptation etc.</li>

</ol>
<hr>

<h3>Rencent News:</h3>
<p>
<ol id="NewsOL">
    <!--    <li>2021-05-08: One co-authored paper was accepted by ICML 2021.</li>-->
    <!--    <li>2021-04-30: One paper was accepted by IJCAI 2021.</li>-->
    <!--    <li>2021-04-06: Invitation to be a technical program committee member (TPC) for NeurIPS 2021.</li>-->
    <!--    <li>2021-03-23: One paper was accepted by IEEE Transactions on Multimedia.</li>-->
    <!--    <li>2021-03-06: One paper was accepted by ICME 2021 for oral representation.</li>-->
    <!--    <li>2020-12-17: One paper was accepted by IEEE Transactions on Knowledge and Data Engineering.</li>-->
    <!--    <li>2020-12-14: I have been invited as a TPC for ICML 2021.</li>-->
    <!--    <li>2020-11-03: I have been invited as an Area Chair for ICME 2021.</li>-->
    <!--    <li>2020-10-28: I have been invited as an Associate Editor for BMC Bioinformatics (IF 3.3).</li>-->
    <!--    <li>2020-10-17: I have been invited to be a senior technical program committee member (SPC) for IJCAI 2021.</li>-->

    <li><p>Attended <b>ChinaMM 2025</b>, 22 - 24 August 2025 • Weihai, Shandong, China • Awarded Best Student Paper Award</p></li>
    <img src="image/news/ChinaMM2025.jpg" width="290px" onclick="window.open('image/news/ACMM2024.jpg', '_blank')">
    
    <li><p>Attended the 32nd  <b>ACM Multimedia</b> Conference, 28 October - 1 November 2024 • Melbourne, Australia</p></li>
    <img src="image/news/ACMM2024.jpg" width="290px" onclick="window.open('image/news/ACMM2024.jpg', '_blank')">
    
    <li><p>Taking part in <b>Valse</b> 2023, 10-12 June 2023 • WuXi, China</p></li>
    <img src="image/news/valse2023.jpg" width="290px" onclick="window.open('image/news/valse2023.jpg', '_blank')">
    
    <li><p>Five papers from our group accepted, February 2023 • One accepted by <b>CVPR 2023</b></p></li>
    <img src="image/news/paper.jpg" width="290px" onclick="window.open('image/news/paper.jpg', '_blank')">
    
    <li><p>Presented at <b>ACM Multimedia</b> 2022, 10-14 October 2022 • Lisbon, Portugal</p></li>
    <img src="image/news/acmm2022.jpg" width="290px" onclick="window.open('image/news/acmm2022.jpg', '_blank')">
    
    <li><p>Taking part in <b>Valse</b> 2021, 8-10 October 2021 • Hangzhou, China</p></li>
    <img src="image/news/valse2021.jpg" width="290px" onclick="window.open('image/news/valse2021.jpg', '_blank')">
    
    <li><p>Presentation at <b>ICIP</b> 2021, 19-22 September 2021 • Anchorage, Alaska, USA</p></li>
    

</ol>
</p>
<hr>

<h3>Published Datesets:</h3>
<table id="tbDatasets">
    <tbody>
    <tr><td><strong>MAFW</strong></td></tr>

    <tr><td><br></td></tr>
    <tr>
        <td width="306">
            <img src="image/database/MAFW.png" width="290px">
        </td>
        <td style="line-height: 120%">
            A large-scale, multi-modal, compound affective database for dynamic facial expression recognition in the wild.<br>
            Contains 10,045 video clips from global regions (China, Japan, Korea, Europe, America, India) across various themes. Each clip was annotated 11 times by trained annotators, featuring:<br>
            • 11-dimensional expression distribution vectors<br>
            • Single/multiple expression labels & bilingual descriptions<br>
            • 11 single-emotion classes and 32 compound-emotion classes<br>
            • Automatic annotations (68 facial landmarks, face bounding boxes)<br>
            Dataset Details： <a href="https://mafw-database.github.io/MAFW/" target="_blank">https://mafw-database.github.io/MAFW/</a><br>
        </td>
    </tr>

    <tr><td><br></td></tr>
    <tr>
        <td width="306"><hr></td>
        <td><hr></td>
    </tr>
    </tbody>
</table>

<h3>Recent Publications:</h3>
<table id="tbPublications" style="width:100%; border-collapse:collapse; margin:20px 0; font-size:14px;">
    <tbody>
        <!-- 2025年论文 -->
        <tr>
            <td colspan="2" style="background:#f5f5f5; padding:10px; border-bottom:2px solid #ddd;">
                <h4 style="margin:0; color:#333;">2025</h4>
            </td>
        </tr>

        <tr style="border-bottom:1px dashed #eee;">
            <td width="306" style="padding:15px; vertical-align:top;">
                <img src="image/journalpaper/ EM-VFER.png" width="290px" style="border:1px solid #eee; border-radius:4px;">
            </td>
            <td style="padding:15px; line-height:1.6;">
                <b>Yuanyuan Liu</b>, Lin Wei, Kejun Liu, Zijing Chen, Zhe Chen*, Chang Tang, Jingying Chen, Shiguang Shan*. "Leveraging Eye Movement for Instructing Robust Video-based Facial Expression Recognition." <i>IEEE Transactions on Affective Computing</i> (2025). 
                [<a href="pdf/journalpaper/Leveraging_Eye_Movement_for_Instructing_Robust_Video-based_Facial_Expression_Recognition.pdf" target="_blank">PDF</a>] 
                [<a href="https://doi.org/10.1109/TAFFC.2025.3599859" target="_blank">DOI: 10.1109/TAFFC.2025.3599859</a>]
                [<a href="https://github.com/934546/EM-VFER" target="_blank">Code (GitHub)</a>]
            </td>
        </tr>

        <tr style="border-bottom:1px dashed #eee;">
            <td width="306" style="padding:15px; vertical-align:top;">
                <img src="image/journalpaper/DAR-SFRL.png" width="290px" style="border:1px solid #eee; border-radius:4px;">
            </td>
            <td style="padding:15px; line-height:1.6;">
                Ke Wang, <b>Yuanyuan Liu*</b>, Chang Tang, Kun Sun, Yibing Zhan, Zhe Chen. "Degradation-adaptive attack-robust self-supervised facial representation learning." <i>Neurocomputing</i> 655: 131356 (2025). 
                [<a href="pdf/journalpaper/2025_Degradation-adaptive attack-robust self-supervised facial representation learning.pdf" target="_blank">PDF</a>] 
                [<a href="https://doi.org/10.1016/j.neucom.2025.131356" target="_blank">DOI: 10.1016/j.neucom.2025.131356</a>]
                [<a href="https://github.com/23wk/DAR-SFRL" target="_blank">Code (GitHub)</a>]
            </td>
        </tr>

        <tr style="border-bottom:1px dashed #eee;">
            <td width="306" style="padding:15px; vertical-align:top;">
                <img src="image/journalpaper/PMTSeg.png" width="290px" style="border:1px solid #eee; border-radius:4px;">
            </td>
            <td style="padding:15px; line-height:1.6;">
                Kejun Liu, Xuesong Yan, <b>Yuanyuan Liu*</b>, Chang Tang, Yibing Zhan, Wei Luo, Wujie Zhou, Hongyan Zhang. "PMTSeg: Prompt-Driven Multimodal Transformer for Task-Adapted Remote Sensing Image Segmentation." <i>IEEE Transactions on Geoscience and Remote Sensing</i> 63: 1-15 (2025)
                [<a href="pdf/journalpaper/2025_PMTSeg Prompt-Driven Multimodal Transformer for Task-Adapted Remote Sensing Image Segmentation.pdf" target="_blank">PDF</a>] 
                [<a href="https://doi.org/10.1109/TGRS.2025.3586620" target="_blank">DOI:10.1109/TGRS.2025.3586620</a>]
            </td>
        </tr>
        
        <tr style="border-bottom:1px dashed #eee;">
            <td width="306" style="padding:15px; vertical-align:top;">
                <img src="image/journalpaper/ECPA.png" width="290px" style="border:1px solid #eee; border-radius:4px;">
            </td>
            <td style="padding:15px; line-height:1.6;">
                Yu Wang, <b>Yuanyuan Liu*</b>, Shunping Zhou, Yuxuan Huang, Chang Tang, Wujie Zhou, Zhe Chen. "Emotion-oriented Cross-modal Prompting and Alignment for Human-centric Emotional Video Captioning." <i>IEEE Transactions on Multimedia</i>  27: 3766-3780 (2025).
                [<a href="pdf/journalpaper/Emotion-oriented Cross-modal Prompting and Alignment for Human-centric Emotional Video Captioning.pdf" target="_blank">PDF</a>] 
                [<a href="https://doi.org/10.1109/TMM.2025.3535292" target="_blank">DOI: 10.1109/TMM.2025.3535292</a>] 
                [<a href="https://github.com/virtuesvvy/ECPA_Emotional_Video_Captioning" target="_blank">Code (GitHub)</a>]
            </td>
        </tr>

        <!-- zongshu -->
        <tr style="border-bottom:1px dashed #eee;">
            <td width="306" style="padding:15px; vertical-align:top;">
                <img src="image/journalpaper/PL综述.png" width="290px" style="border:1px solid #eee; border-radius:4px;">
            </td>
            <td style="padding:15px; line-height:1.6;">
                <b>刘袁缘</b>,  刘树阳,  刘云娇,  袁雨晨,  唐厂,  罗威.  提示学习在计算机视觉中的分类、应用及展望.  <i>自动化学报</i>,  2025, 51(5): 1021−1040
                [<a href="pdf/journalpaper/提示学习在计算机视觉中的分类、应用及展望.pdf" target="_blank">PDF</a>] 
                [<a href="https://doi.org/10.16383/j.aas.c240177" target="_blank">DOI: 10.16383/j.aas.c240177</a>] 

            </td>
        </tr>

        <!-- zhouning -->
        <tr style="border-bottom:1px dashed #eee;">
            <td width="306" style="padding:15px; vertical-align:top;">
                <img src="image/journalpaper/HC-UTAL.png" width="290px" style="border:1px solid #eee; border-radius:4px;">
            </td>
            <td style="padding:15px; line-height:1.6;">
                <b>Yuanyuan Liu</b>, Ning Zhou, Yuxuan Huang, Shuyang Liu, Leyuan Liu, Wujie Zhou, Chang Tang, Ke Wang*. "Beyond boundaries: Hierarchical-contrast unsupervised temporal action localization with high-coupling feature learning.<i>Pattern Recognition</i> 162: 111421 (2025)
                [<a href="pdf/journalpaper/2025_Beyond boundaries Hierarchical-contrast unsupervised temporal action localization with high-couplin.pdf" target="_blank">PDF</a>] 
                [<a href="https://doi.org/10.1016/j.patcog.2025.111421" target="_blank">DOI: 10.1016/j.patcog.2025.111421</a>]
                [<a href="https://github.com/bugcat9/HC-UTAL" target="_blank">Code (GitHub)</a>]
            </td>
        </tr>

        <tr style="border-bottom:1px dashed #eee;">
            <td width="306" style="padding:15px; vertical-align:top;">
                <img src="image/journalpaper/PCFRL.png" width="290px" style="border:1px solid #eee; border-radius:4px;">
            </td>
            <td style="padding:15px; line-height:1.6;">
                <b>Yuanyuan Liu</b>, Shaoze Feng, Shuyang Liu, Yibing Zhan, Dapeng Tao, Zijing Chen, Zhe Chen*. "Sample-Cohesive Pose-Aware Contrastive Facial Representation Learning." <i>International Journal of Computer Vision</i> 133(6): 3727-3745 (2025). 
                [<a href="pdf/journalpaper/Sample-Cohesive Pose-Aware Contrastive Facial Representation Learning.pdf" target="_blank">PDF</a>] 
                [<a href="https://doi.org/10.1007/s11263-025-02348-z" target="_blank">DOI: 10.1007/s11263-025-02348-z</a>] 
                [<a href="https://github.com/fulaoze/CV/tree/main" target="_blank">Code (GitHub)</a>]
            </td>
        </tr>

        <tr style="border-bottom:1px dashed #eee;">
            <td width="306" style="padding:15px; vertical-align:top;">
                <img src="image/journalpaper/NRGF.png" width="290px" style="border:1px solid #eee; border-radius:4px;">
            </td>
            <td style="padding:15px; line-height:1.6;">
                <b>Yuanyuan Liu</b>, Haoyu Zhang*, Yibing Zhan, Zijing Chen, Guanghao Yin, Lin Wei, Zhe Chen. "Noise-Resistant Multimodal Transformer for Emotion Recognition." <i>International Journal of Computer Vision</i> 133(5): 3020-3040 (2025). 
                [<a href="pdf/journalpaper/Noise-Resistant Multimodal Transformer for Emotion Recognition.pdf" target="_blank">PDF</a>] 
                [<a href="https://doi.org/10.1007/s11263-024-02304-3" target="_blank">DOI: 10.1007/s11263-024-02304-3</a>]
            </td>
        </tr>

        <!-- 会议 -->
        <tr style="border-bottom:1px dashed #eee;">
            <td width="306" style="padding:15px; vertical-align:top;">
                <img src="image/journalpaper/IJCNN.png" width="290px" style="border:1px solid #eee; border-radius:4px;">
            </td>
            <td style="padding:15px; line-height:1.6;">
                Lin Wei, <b>Yuanyuan Liu*</b>, Kejun Liu, Yuxuan Huang, Shaozhe Feng. "Emotion-Aware Text Generation for Instructing Audio-Visual Emotion Recognition." <i>IEEE IJCNN.</i> (2025).
                [<a href="pdf/journalpaper/Emotion-oriented Cross-modal Prompting and Alignment for Human-centric Emotional Video Captioning.pdf" target="_blank">PDF</a>] 
                <!-- [<a href="https://doi.org/" target="_blank">DOI: </a>]  -->
            </td>
        </tr>



        <!-- 2024年论文 -->
        <tr>
            <td colspan="2" style="background:#f5f5f5; padding:10px; border-bottom:2px solid #ddd;">
                <h4 style="margin:0; color:#333;">2024</h4>
            </td>
        </tr>

        <tr style="border-bottom:1px dashed #eee;">
            <td width="306" style="padding:15px; vertical-align:top;">
                <img src="image/journalpaper/Token-disentangling.png" width="290px" style="border:1px solid #eee; border-radius:4px;">
            </td>
            <td style="padding:15px; line-height:1.6;">
                Guanghao Yin*, <b>Yuanyuan Liu</b>, Tengfei Liu, Haoyu Zhang, Fang Fang, Chang Tang, Liangxiao Jiang. "Token-disentangling Mutual Transformer for Multimodal Emotion Recognition Engineering." <i>Applications of Artificial Intelligence</i>, Volume 133, Part D, July 2024, Page 108348. 
                [<a href="pdf/journalpaper/Token-disentangling.pdf" target="_blank">PDF</a>] 
                [<a href="https://www.sciencedirect.com/science/article/pii/S0952197624005062" target="_blank">DOI: 10.1016/j.engappai.2024.108348</a>] 
                [<a href="https://github.com/cug-ygh/TMT" target="_blank">Code (GitHub)</a>]
            </td>
        </tr>

        <tr style="border-bottom:1px dashed #eee;">
            <td width="306" style="padding:15px; vertical-align:top;">
                <img src="image/journalpaper/Multitarget_Domain_Adaptation_Building_Instance_Extraction_of_Remote_Sensing_Imagery_With_Domain-Common_Approximation_Learning.png" width="290px" style="border:1px solid #eee; border-radius:4px;">
            </td>
            <td style="padding:15px; line-height:1.6;">
                Fayong Zhang, Kejun Liu, <b>Yuanyuan Liu*</b>, Chaofan Wang, Wujie Zhou, Hongyan Zhang, Lizhe Wang. "Multi-target Domain Adaptation Building Instance Extraction of Remote Sensing Imagery with Domain-common Approximation Learning." <i>IEEE Transactions on Geoscience and Remote Sensing</i> 62: 1-16 (2024). 
                [<a href="pdf/journalpaper/Multitarget_Domain_Adaptation_Building_Instance_Extraction_of_Remote_Sensing_Imagery_With_Domain-Common_Approximation_Learning.pdf" target="_blank">PDF</a>] 
                [<a href="https://www.sciencedirect.com/science/article/pii/S0952197624005062" target="_blank">DOI: 10.1109/TGRS.2024.3376719</a>] 
                [<a href="https://github.com/kejun1/DAL/tree/main" target="_blank">Code (GitHub)</a>]
            </td>
        </tr>
        <!-- wangchaofan -->
        <tr style="border-bottom:1px dashed #eee;">
            <td width="306" style="padding:15px; vertical-align:top;">
                <img src="image/journalpaper/MDMT.png" width="290px" style="border:1px solid #eee; border-radius:4px;">
            </td>
            <td style="padding:15px; line-height:1.6;">
                <b>刘袁缘</b>, 王超凡, 王文斌, 张浩宇, 罗忠文*. 面向多种天气场景下目标检测的多域动态平均教师模型. <i>计算机辅助设计与图形学学报</i>, 2024, 36(3): 388-398.
                [<a href="pdf/journalpaper/面向多种天气场景下目标检测的多域动态平均教师模型.pdf" target="_blank">PDF</a>] 
                [<a href="https://doi.org/10.3724/SP.J.1089.2024.19820" target="_blank">DOI: 10.3724/SP.J.1089.2024.19820</a>] 
            </td>
        </tr>

        <!-- 会议 -->
        <tr style="border-bottom:1px dashed #eee;">
            <td width="306" style="padding:15px; vertical-align:top;">
                <img src="image/conferencepaper/ftp.png" width="290px" style="border:1px solid #eee; border-radius:4px;">
            </td>
            <td style="padding:15px; line-height:1.6;">
                <b>Yuanyuan Liu</b>, Yuxuan Huang*, Shuyang Liu, Yibing Zhan, Zijing Chen, Zhe Chen. "Open-Set Video-based Facial Expression Recognition with Human Expression-sensitive Prompting." In <i>Proceedings of the 32nd ACM International Conference on Multimedia (ACM MM 2024)</i>, Melbourne, Australia, October 2024. 
                [<a href="pdf/conferencepaper/ftp4871 camera-ready.pdf" target="_blank">PDF</a>] 
                [<a href="https://github.com/cosinehuang/HESP" target="_blank">Code (GitHub)</a>] 
                [<a href="pdf/conferencepaper/ftp4871 Supplementary.pdf" target="_blank">Supplementary</a>]
            </td>
        </tr>

        <!-- 2023年论文 -->
        <tr>
            <td colspan="2" style="background:#f5f5f5; padding:10px; border-bottom:2px solid #ddd;">
                <h4 style="margin:0; color:#333;">2023</h4>
            </td>
        </tr>


        <tr style="border-bottom:1px dashed #eee;">
            <td width="306" style="padding:15px; vertical-align:top;">
                <img src="image/journalpaper/APSL Action-positive separation learning for unsupervised temporal action localization.png" width="290px" style="border:1px solid #eee; border-radius:4px;">
            </td>
            <td style="padding:15px; line-height:1.6;">
                <b>Yuanyuan Liu</b>, Ning Zhou, Fayong Zhang, Wenbin Wang, Yu Wang, Kejun Liu, Ziyuan Liu. "APSL: Action-positive Separation Learning for Unsupervised Temporal Action Localization." <i>Information Sciences</i>, Volume 630, 2023, Pages 206-221. 
                [<a href="pdf/journalpaper/APSL Action-positive separation learning for unsupervised temporal action localization.pdf" target="_blank">PDF</a>] 
                [<a href="https://doi.org/10.1016/j.ins.2023.02.047" target="_blank">DOI: 10.1016/j.ins.2023.02.047</a>] 
                [<a href="https://github.com/bugcat9/APSL" target="_blank">Code (GitHub)</a>]
            </td>
        </tr>

        <tr style="border-bottom:1px dashed #eee;">
            <td width="306" style="padding:15px; vertical-align:top;">
                <img src="image/journalpaper/Expression Snippet Transformer for Robust Video-based Facial Expression Recognition.png" width="290px" style="border:1px solid #eee; border-radius:4px;">
            </td>
            <td style="padding:15px; line-height:1.6;">
                <b>Yuanyuan Liu</b>, Wenbin Wang, Chuanxu Feng, Haoyu Zhang, Zhe Chen, Yibing Zhan. "Expression Snippet Transformer for Robust Video-based Facial Expression Recognition." <i>Pattern Recognition</i>, Volume 138, 2023, Page 109368. 
                [<a href="pdf/journalpaper/Expression Snippet Transformer for Robust Video-based Facial Expression Recognition.pdf" target="_blank">PDF</a>] 
                [<a href="https://doi.org/10.1016/j.patcog.2023.109368" target="_blank">DOI: 10.1016/j.patcog.2023.109368</a>] 
                [<a href="https://github.com/DreamMr/EST" target="_blank">Code (GitHub)</a>]
            </td>
        </tr>

        <tr style="border-bottom:1px dashed #eee;">
            <td width="306" style="padding:15px; vertical-align:top;">
                <img src="image/conferencepaper/PCL.png" width="290px" style="border:1px solid #eee; border-radius:4px;">
            </td>
            <td style="padding:15px; line-height:1.6;">
                <b>Yuanyuan Liu</b>, Wenbin Wang, Yibing Zhan, Shaoze Feng, Kejun Liu, Zhe Chen. "Pose-disentangled Contrastive Learning for Self-supervised Facial Representation." In <i>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2023)</i>, Vancouver, Canada, June 2023. 
                [<a href="pdf/conferencepaper/Liu_Pose-Disentangled_Contrastive_Learning_for_Self-Supervised_Facial_Representation_CVPR_2023_paper.pdf" target="_blank">PDF</a>] 

            </td>
        </tr>

        <!-- zhanghaoyu -->
        <tr style="border-bottom:1px dashed #eee;">
            <td width="306" style="padding:15px; vertical-align:top;">
                <img src="image/conferencepaper/ALMT.png" width="290px" style="border:1px solid #eee; border-radius:4px;">
            </td>
            <td style="padding:15px; line-height:1.6;">
                Haoyu Zhang, Yu Wang, Guanghao Yin, Kejun Liu, <b>Yuanyuan Liu</b>, Tianshu Yu*. <i>EMNLP</i>  2023: 756-767.
                [<a href="pdf/conferencepaper/2023_Learning Language-guided Adaptive Hyper-modality Representation for Multimodal Sentiment Analysis.pdf" target="_blank">PDF</a>] 

            </td>
        </tr>

        <!-- 2022年论文 -->
        <tr>
            <td colspan="2" style="background:#f5f5f5; padding:10px; border-bottom:2px solid #ddd;">
                <h4 style="margin:0; color:#333;">2022</h4>
            </td>
        </tr>
        <!-- 期刊 -->
        <tr style="border-bottom:1px dashed #eee;">
            <td width="306" style="padding:15px; vertical-align:top;">
                <img src="image/journalpaper/ConGNN%20Context-consistent%20cross-graph%20neural%20network%20for%20group%20emotion%20recognition%20in%20the%20wild.jpg" width="290px" style="border:1px solid #eee; border-radius:4px;">
            </td>
            <td style="padding:15px; line-height:1.6;">
                Yu Wang, Shunping Zhou, <b>Yuanyuan Liu*</b>, Kunpeng Wang, Fang Fang, Haoyue Qian. "ConGNN: Context-consistent Cross-graph Neural Network for Group Emotion Recognition in the Wild." <i>Information Sciences</i> 610: 707-724 (2022). 
                [<a href="pdf/journalpaper/ConGNN%20Context-consistent%20cross-graph%20neural%20network%20for%20group%20emotion%20recognition%20in%20the%20wild.pdf" target="_blank">PDF</a>] 
                [<a href="https://doi.org/10.1016/j.ins.2022.08.003" target="_blank">DOI: 10.1016/j.ins.2022.08.003</a>] 
                [<a href="https://github.com/virtuesvvy/ConGNN-master" target="_blank">Code (GitHub)</a>] 
                [<a href="https://pan.baidu.com/s/1gzxyvysfVUdFn73cA89-DA?pwd=u3f9" target="_blank">SiteGroEmo Dataset</a>]<br>
                <b>Note</b>: The data are passcode protected. Please download and send the signed [<a href="https://pan.baidu.com/s/1NLCBMN5TGumfQpjsGSS2Vw?pwd=itgz" target="_blank">EULA</a>] to <a href="mailto:qianying@cug.edu.cn">qianying@cug.edu.cn</a> for access request.
            </td>
        </tr>

        <tr style="border-bottom:1px dashed #eee;">
            <td width="306" style="padding:15px; vertical-align:top;">
                <img src="image/journalpaper/Clip-aware Expressive Feature Learning for Video-based Facial Expression.png" width="290px" style="border:1px solid #eee; border-radius:4px;">
            </td>
            <td style="padding:15px; line-height:1.6;">
                <b>Yuanyuan Liu</b>, Chuanxu Feng, Xiaohui Yuan*, Lin Zhou, Wenbin Wang, Jie Qin, Zhongwen Luo. "Clip-aware Expressive Feature Learning for Video-based Facial Expression Recognition." <i>Information Sciences</i> 598: 182-195 (2022). 
                [<a href="pdf/journalpaper/Clip-aware Expressive Feature Learning for Video-based Facial Expression.pdf" target="_blank">PDF</a>] 
                [<a href="https://doi.org/10.1016/j.ins.2022.03.062" target="_blank">DOI: 10.1016/j.ins.2022.03.062</a>] 
                [<a href="https://github.com/CVLab-Liuyuanyuan/CEFLNet" target="_blank">Code (GitHub)</a>]
            </td>
        </tr>

        <tr style="border-bottom:1px dashed #eee;">
            <td width="306" style="padding:15px; vertical-align:top;">
                <img src="image/conferencepaper/MAFW%20A%20Large-scale,%20Multi-modal,%20Compound%20Affective%20Database%20for%20Dynamic%20Facial%20Expression%20Recognition%20in%20the%20Wild.png" width="290px" style="border:1px solid #eee; border-radius:4px;">
            </td>
            <td style="padding:15px; line-height:1.6;">
                <b>Yuanyuan Liu</b>, Wei Dai, Chuanxu Feng, Wenbin Wang, Guanghao Yin, Jiabei Zeng, Shiguang Shan*. "MAFW: A Large-scale, Multi-modal, Compound Affective Database for Dynamic Facial Expression Recognition in the Wild." In <i>Proceedings of the 30th ACM International Conference on Multimedia (ACM MM 2022)</i> (Oral Presentation), Lisbon, Portugal, October 2022. 
                [<a href="pdf/conferencepaper/MAFW%20A%20Large-scale,%20Multi-modal,%20Compound%20Affective%20Database%20for%20Dynamic%20Facial%20Expression%20Recognition%20in%20the%20Wild.pdf" target="_blank">PDF</a>] 
                [<a href="https://mafw-database.github.io/MAFW/" target="_blank">MAFW Database URL</a>] 
                [<a href="pdf/conferencepaper/MAFW_supp.pdf" target="_blank">Supplementary</a>]
            </td>
        </tr>

        <!-- 2021年论文 -->
        <tr>
            <td colspan="2" style="background:#f5f5f5; padding:10px; border-bottom:2px solid #ddd;">
                <h4 style="margin:0; color:#333;">2021</h4>
            </td>
        </tr>
        <!-- 期刊 -->
        <tr style="border-bottom:1px dashed #eee;">
            <td width="306" style="padding:15px; vertical-align:top;">
                <img src="image/journalpaper/DML-Net.png" width="290px" style="border:1px solid #eee; border-radius:4px;">
            </td>
            <td style="padding:15px; line-height:1.6;">
                <b>Yuanyuan Liu</b>, Wei Dai, Fang Fang, Yongquan Chen*, Rui Huang, et al. "Dynamic Multi-channel Metric Network for Joint Pose-aware and Identity-invariant Facial Expression Recognition." <i>Information Sciences</i>, Volume 578, 2021. 
                [<a href="pdf/journalpaper/DML-Net.pdf" target="_blank">PDF</a>] 
                [<a href="https://doi.org/10.1016/j.ins.2021.07.034" target="_blank">DOI: 10.1016/j.ins.2021.07.034</a>]
            </td>
        </tr>

        <tr style="border-bottom:1px dashed #eee;">
            <td width="306" style="padding:15px; vertical-align:top;">
                <img src="image/journalpaper/remotesense.png" width="290px" style="border:1px solid #eee; border-radius:4px;">
            </td>
            <td style="padding:15px; line-height:1.6;">
                <b>Yuanyuan Liu</b>, Wenbin Wang, Fang Fang, Lin Zhou, Chenxing Sun, Ying Zheng, Zhanlong Chen*. "CscGAN: Conditional Scale-Consistent Generation Network for Multi-Level Remote Sensing Image to Map Translation." <i>Remote Sensing</i>, Volume 13, No. 10, 2021, Page 1936. 
                [<a href="pdf/journalpaper/remotesensing-13-01936-v2_.pdf" target="_blank">PDF</a>] 
                [<a href="https://doi.org/10.3390/rs13101936" target="_blank">DOI: 10.3390/rs13101936</a>]
            </td>
        </tr>

        <tr style="border-bottom:1px dashed #eee;">
            <td width="306" style="padding:15px; vertical-align:top;">
                <img src="image/journalpaper/TSingNet.jpg" width="290px" style="border:1px solid #eee; border-radius:4px;">
            </td>
            <td style="padding:15px; line-height:1.6;">
                <b>Yuanyuan Liu</b>, Jiyao Peng, Jing-Hao Xue, Yongquan Chen, Zhang-Hua Fu. "TSingNet: Scale-aware and Context-rich Feature Learning for Traffic Sign Detection and Recognition in the Wild." <i>Neurocomputing</i>, Volume 447, Issue 4, 2021. 
                [<a href="pdf/journalpaper/2021_TSingNet Scale-aware and context-rich feature learning for traffic sign detection and recognition i.pdf" target="_blank">pdf</a>]
                [<a href="https://doi.org/10.1016/j.neucom.2021.03.049" target="_blank">doi</a>]
                [<a href="https://pan.baidu.com/s/10XLQDjzRt3fdLnwBLHkusA" target="_blank">Code (Baidu Cloud, Password: code)</a>] 
                [<a href="https://github.com/Lionel-McCree/TsingNet" target="_blank">Code (GitHub)</a>]
            </td>
        </tr>
        <!-- 会议 -->
        <tr style="border-bottom:1px dashed #eee;">
            <td width="306" style="padding:15px; vertical-align:top;">
                <img src="image/conferencepaper/HIERARCHICAL%20DOMAIN-CONSISTENT%20NETWORK.png" width="290px" style="border:1px solid #eee; border-radius:4px;">
            </td>
            <td style="padding:15px; line-height:1.6;">
                <b>Yuanyuan Liu</b>, Ziyang Liu, Fang Fang*, Zhanghua Fu, Zhanlong Chen. "Hierarchical Domain-Consistent Network for Cross-Domain Object Detection." In <i>Proceedings of the IEEE International Conference on Image Processing (ICIP 2021)</i>, Anchorage, Alaska, USA, September 2021. 
                [<a href="pdf/conferencepaper/HIERARCHICAL%20DOMAIN-CONSISTENT%20NETWORK.pdf" target="_blank">PDF</a>]
            </td>
        </tr>

        <!-- 2020年论文 -->
        <tr>
            <td colspan="2" style="background:#f5f5f5; padding:10px; border-bottom:2px solid #ddd;">
                <h4 style="margin:0; color:#333;">2020</h4>
            </td>
        </tr>
        <!-- 期刊 -->
        <tr style="border-bottom:1px dashed #eee;">
            <td width="306" style="padding:15px; vertical-align:top;">
                <img src="image/journalpaper/multi-scale%20u-shaped%20cnn.png" width="290px" style="border:1px solid #eee; border-radius:4px;">
            </td>
            <td style="padding:15px; line-height:1.6;">
                <b>Yuanyuan Liu</b>, Dingyuan Chen*, Ailong Ma*, Yanfei Zhong, Fang Fang, Kai Xu. "A Multi-Scale U-Shaped CNN Building Instance Extraction Framework With Edge Constraint for High Spatial Resolution Remote Sensing Imagery." <i>IEEE Transactions on Geoscience and Remote Sensing</i>, 2020, DOI: 10.1109/TGRS.2020.3022410. 
                [<a href="pdf/journalpaper/multi-scale%20u-shaped%20cnn.pdf" target="_blank">PDF</a>]
            </td>
        </tr>

        <!-- 2018年论文 -->
        <tr>
            <td colspan="2" style="background:#f5f5f5; padding:10px; border-bottom:2px solid #ddd;">
                <h4 style="margin:0; color:#333;">2018</h4>
            </td>
        </tr>
        <!-- 期刊 -->
        <tr style="border-bottom:1px dashed #eee;">
            <td width="306" style="padding:15px; vertical-align:top;">
                <img src="image/journalpaper/conditional%20cnn%20etc.jpg" width="290px" style="border:1px solid #eee; border-radius:4px;">
            </td>
            <td style="padding:15px; line-height:1.6;">
                <b>Yuanyuan Liu</b>, Xiaohui Yuan*, Xi Gong, Zhong Xie, Fang Fang, Zhongwen Luo. "Conditional Convolution Neural Network Enhanced Random Forest for Facial Expression Recognition." <i>Pattern Recognition</i>, Volume 84, 2018, Pages 251-261. 
                [<a href="pdf/journalpaper/conditional%20cnn%20etc.pdf" target="_blank">PDF</a>] 
                [<a href="https://doi.org/10.1016/j.patcog.2018.07.016" target="_blank">DOI: 10.1016/j.patcog.2018.07.016</a>]
            </td>
        </tr>
        

        <!-- 会议 -->
        <tr style="border-bottom:1px dashed #eee;">
            <td width="306" style="padding:15px; vertical-align:top;">
                <img src="image/conferencepaper/Multi-Channel%20Pose-Aware%20Convolution%20Neural%20Networks.jpg" width="290px" style="border:1px solid #eee; border-radius:4px;">
            </td>
            <td style="padding:15px; line-height:1.6;">
                <b>Yuanyuan Liu</b>, Jiabei Zeng, Shiguang Shan*, Zuo Zheng. "Multi-Channel Pose-Aware Convolution Neural Networks for Multi-View Facial Expression Recognition." In <i>Proceedings of the 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)</i>, Xi'an, China, May 2018, Pages 458-464. 
                [<a href="pdf/conferencepaper/Multi-Channel%20Pose-Aware%20Convolution%20Neural%20Networks.pdf" target="_blank">PDF</a>]
            </td>
        </tr>
    </tbody>
</table>
<br>
<hr>  


<!-- <h3>Research Project (PI)</h3>
<p>
<ol id="ProjectsOL" style="line-height: 150%">
    <li>Jan.2021~Dec.2024, “Research on Video Emotion Detection and Recognition Method under Unconstrained Conditions”, Sponsored by National Science Foundation (No. 62076227), PI</li>
    <li>Sep.2020~Dec.2023, “Key technologies and applications of video-based intelligent emotional computing”, Sponsored by Wuhan Basic Frontier Project of Science and Technology , PI</li>
    <li>Sep.2019~Dec.2020, “Intelligent rapid recognition technology of typical targets based on few-shot learning”, Sponsored by Aviation Science and Industry Joint Fund Project, PI</li>
    <li>Jan.2016~Dec.2019, “Research on spontaneous expression recognition based on deep enhanced random forests under multi-noises”, Sponsored by National Science Foundation (No. 61602429), PI</li>
    <li>Jan.2016~Dec.2018, “Research on spontaneous facial expression recognition under multi pose and occlusion conditions”, Supported by China Postdoctoral Science Foundation. (No. 2016M592406), PI</li>
    <li>Jan.2016~Jun.2018, “Research on human behavior recognition, tracking and prediction in infrared videos”, Development Projects, Sponsored by the Fundamental Research Funds for the Central Universities, China University of Geosciences (Wuhan) (No. 26420160055), PI</li>
</ol>
</p> -->





<h3>PROFESSIONAL ACTIVITIES:</h3>
<p><em>Academic Appointments</em></p>
<table class="activities-table">
    <tbody>
    <tr>
        <td><b>Associate Researcher</b>, Institute of Robotics and Artificial Intelligence, The Chinese University of Hong Kong, Shenzhen</td>
    </tr>
    <tr>
        <td><b>Associate Editor</b>, IEEE Transactions on Multimedia (TMM)</td>
    </tr>
    <tr>
        <td><b>Guest Editor</b>, Applied Sciences</td>
    </tr>
    </tbody>
</table>

<p><em>Paper Reviewer</em></p>
<table class="activities-table">
    <tbody>
    <tr>
        <td><b>Journals:</b> IJCV, TKDE, TIP, TASE, PR, IJRS, TGRS, FCS, IEEE Access, MTA, etc.</td>
    </tr>
    <tr>
        <td><b>Conferences:</b> CVPR, ECCV, ICCV, ACM MM, AAAI, IJCAI</td>
    </tr>
    </tbody>
</table>

<hr>
<h3>Team Member</h3>
<table width="100%">
    <tbody>
    <tr><td><br></td></tr>
    <tr align="center">
        <td width="16.6%"><a href=""><img src="image/member/liukejun.jpg" width="100%">Liu Kejun</a></td>
        <td width="16.6%"><a href=""><img src="image/member/wk.jpg" width="100%">Wang Ke</a></td>
        <td width="16.6%"><a href=""><img src="image/member/lsy.jpg" width="100%">Liu Shuyang</a></td>
        <td width="16.6%"><a href=""><img src="image/member/lyj.jpg" width="100%">Liu Yunjiao</a></td>
        <td width="16.6%"><a href=""><img src="image/member/wyc.jpg" width="100%">Yuan Yuchen</a></td>
        <td width="16.6%"><a href=""><img src="image/member/qianying.jpg" width="100%">Qian Ying</td>
    </tr>
    <tr><td><br></td></tr>
    <tr align="center">
        <td width="16.6%"><a href=""><img src="image/member/xulei.png" width="100%">Xu Lei</td>
        <td width="16.6%"><a href=""><img src="image/member/zhouqiyin.jpg" width="100%">Zhou Qiyin</td>
        <td width="16.6%"><a href=""><img src="image/member/kelv.jpg" width="100%">Ke Lv</td>
        <td width="16.6%"><a href=""><img src="image/member/zhangjiahao.jpg" width="100%">Zhang Jiahao</td>
        <td width="16.6%"><a href=""><img src="image/member/xiayuyang.jpg" width="100%">Xia Yuyang</td>
        <td width="16.6%"><a href=""><img src="image/member/pengjiyao.jpg" width="100%">Peng Jiyao（guaduated）</a></td> 
    </tr>
    <tr><td><br></td></tr>
    <tr align="center">
        <td width="16.6%"><a href=""><img src="image/member/daiwei.png" width="100%">Dai Wei（guaduated）</a></td>
        <td width="16.6%"><a href=""><img src="image/member/wangyu.jpg" width="100%">Wang Yu（guaduated）</a></td>
        <td width="16.6%"><a href=""><img src="image/member/liuziyang.png" width="100%">Liu Ziyang（guaduated）</a></td>
        <td width="16.6%"><a href=""><img src="image/member/wangkunpeng.jpg" width="100%">Wang Kunpeng（guaduated）</a></td>
        <td width="16.6%"><a href=""><img src="image/member/fengchuanxu.png" width="100%">Feng Chuanxu（guaduated）</a></td>
        <td width="16.6%"><a href=""><img src="image/member/wangwenbin.jpg" width="100%">Wang Wenbin（guaduated）</a></td>

    </tr>
    <tr><td><br></td></tr>
    <tr align="center">
        <td width="16.6%"><a href=""><img src="image/member/zhanghaoyu.jpg" width="100%">Zhang Haoyu（guaduated）</a></td>
        <td width="16.6%"><a href=""><img src="image/member/wangchaofan.jpg" width="100%">Wang Chaofan（guaduated）</a></td>
        <td width="16.6%"><a href=""><img src="image/member/zhouning.jpg" width="100%">Zhou Ning（guaduated）</a></td>
        <td width="16.6%"><a href=""><img src="image/member/yinguanghao.jpg" width="100%">Yin Guanghao（guaduated）</a></td>
        <td width="16.6%"><a href=""><img src="image/member/lkl.jpg" width="100%">Li Kanglin（guaduated）</a></td>
        <td width="16.6%"><a href=""><img src="image/member/zhaoyaowu.jpg" width="100%">Zhao Yaowu（guaduated）</a></td>
    </tr>
    
    <tr><td><br></td></tr>
    <tr align="center">
        <td width="16.6%"><a href=""><img src="image/member/fengshaoze.jpg" width="100%">Feng Shaoze（guaduated）</a></td>
        <td width="16.6%"><a href=""><img src="image/member/huangyuxuan.jpg" width="100%">Huang Yuxuan（guaduated）</a></td>
        <td width="16.6%"><a href=""><img src="image/member/weilin.jpg" width="100%">Wei Lin（guaduated）</a></td>
    </tr>
    </tbody>
</table>

<hr>
<br>
<h4>WEBSITE VISITING STATISTICS</h4>
<table>
    <tbody>
    <tr>
        <td>
            <a href="https://clustrmaps.com/site/1birr"  title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=N2t4JJQU4CTbbZSYoeGo7EdtfN81cu4m3ki6BB3Fc68&cl=ffffff" /></a>
        </td>
    </tr>
    </tbody>
</table>
<br>

<!-- <textarea name="" id="writeArea"></textarea>
<button>发布</button>
<ul>

</ul>

<script>
    var text = document.querySelector("textarea");
    var btn = document.querySelector("button");
    var ul = document.querySelector("ul");

    btn.onclick = function () {
    //   获取textarea的值
        var val = text.value;
        // 如果值为空不添加
        if (val == "") {
          alert("请先输入内容");
          //   函数里面终止用return，break在函数外用
          return;
        }
        // 创建一个li
        var li = document.createElement("li");
        // 让li的值等于textarea的值
        li.innerText = val;
        // 把li增加到ul里面,插入到第一行
        ul.insertBefore(li, ul.children[0]);
        // 添加内容之后内容清空
        text.value = "";
    };

</script> -->


<script>
    function displayTime(){
        var elt = document.getElementById("clock");
        var now = new Date();
        elt.innerHTML  = now.toLocaleDateString() + " " + now.toLocaleTimeString();
        setTimeout(displayTime,1000);
        }
    window.onload = displayTime;
</script>
<style>
#clock{
    font: bold 15pt sans;
    background: rgba(238, 237, 240, 0.507);
    padding: 10px;
    border: solid rgb(170, 170, 170) 2px;
    border-radius: 10px;
    display: inline-block;
    margin: 0 auto;}
</style>


</body></html>











